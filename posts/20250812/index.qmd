---
title: "Remembering My Target Audience"
author: "Tony Dunsworth, Ph.D."
date: "2025-08-14"
categories: [development, software]
execute: 
  eval: false
---

I like soliciting feedback for this blog so I can refine my work and ensure that I'm reaching my target audience effectively. I received some valuable feedback recently that made me reflect on my target audience and how I communicate with them. I meant for this to appeal to professionals in the 9-1-1 and public safety fields, along with people interested in statistics, analytics, and data science overall. The feedback I received after my most recent post suggested that I needed to reflect on my target audience more carefully.

My feedback came from two highly intelligent professionals, both [Toastmasters](https://www.toastmasters.org) for whom I have deep respect. One is a 9-1-1 professional and the other a retired medical professional. Both of them gave me similar feedback. They both enjoy my writing style and felt that I write well. However, they also pointed out that I lost them in some of the technical details. I went a little too deep into the details. I think that's because I haven't deployed my framework to GitHub yet.

So, I want to back my own bus up a bit, so to speak, and recast my last post in a little better format. I think this will reach my target audience more effectively and, hopefully, make the technical choices clearer to my actual target audience.

While you can perform data analysis in many different languages and you can do a fair piece of data analysis in Excel, there are three main choices, programmatically, to do basic, intermediate, and advanced data analysis. These are [Python](https://www.python.org), [R](https://cran.r-project.org/), and [Julia](https://julialang.org/). R has a reputation for being a great statistical programming language and can be closely associated with academia. Julia is the new kid on the block and has a lot of promise. However, I don't see as much work daily work in it. If you are interested in Julie, I recommend reading [Emma Boudreau's Medium Blog](https://medium.com/@emmacode) and following her [GitHub Repositories](https://github.com/emmaccode). They give an excellent view of what Julia can do beyond basic data analysis. Python is an excellent general programming language with a lot of supplemental libraries that can speed up its performance and make it an excellent choice for data analysis, data engineering, machine learning modelling, and even Large Language Model (LLM), and AI development. So I chose Python because of the libraries I know that I need as well as ones that I think I will need as the project grows through the phases that I've envisioned for it.

Because a typical Python project requires several libraries, there is always a high-likelihood that some of the libraries will not play well together at the most up-to-date version of Python. Currently, the most recent release according to the python.org website is 3.13.13. Many of them will work together at some earlier version, so a key piece of building the infrastructure for the project is version management and control. There are, obviously, many ways to do that. I've tried several different methods and I've settled on one that really works well for me. I use [uv](https://docs.astral.sh/uv/) by [Astral Software](https://astral.sh/). I also use their linting and formatting tool [Ruff](https://docs.astral.sh/ruff/), but that will be discussed a little bit later. I chose it because, in many cases, you can, after installation, use many of the same [pip](https://pypi.org/project/pip/) or [venv](https://docs.python.org/3.11/library/venv.html) commands to configure your environment and install libraries and only preface them on the command line with *uv*. Astral wrote uv in [Rust](https://www.rust-lang.org/) which is known for creating very fast software. This speeds up Python's built-in tools and expands your capabilities by adding commands like `{bash} $ uv tool install ruff` or `{bash} $ uvx ruff` to install tools and executing scripts with a command like `{bash} $ uv run synth911gen.py`. 

Other environments, like [Anaconda](https://anaconda.com), provide full environments, but the libraries can lag behind and it is a total control mechanism. So, if you export the environment to another computer, it has to have Anaconda installed also. [Poetry](https://python-poetry.org/), like uv, supports greater collaboration since the environment can be packaged in a single [toml](https://toml.io/en/) file and shipped to another workstation. From there, both can use their version of toml to synchronize the environment and download the necessary libraries with all the needed dependencies. I just found uv easier to use and less argumentative than Poetry. Having said that, remember that your milage may vary and if one works better for you than the other, by all means, use what works best for you. I know there may be readers who will ask if you can do most of this by prepending 'uv' in front of other Python commands, why not just use pip and venv? My answer is that it works quickly and seems more consistent to me.

Once the environment is set, then I ensure that I install a version of Python for that environment. Right now, I've stayed with the latest patch to 3.11 which is 3.11.13. It has an end of life date in 2027, so I can work with it and I know that it is still in support while I build the project. Most of the libraries that I've chosen are compatible with it and with each other at that version. I have considered moving to 3.12, but I haven't tested that all of my libraries will work together and with 3.12. As I move forward in the project, I will test version and patch levels to see if everything will work as I would like it to. 