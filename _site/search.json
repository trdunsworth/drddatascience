[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Resume for Tony Dunsworth Ph.D.\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "posts/20250129/index.html",
    "href": "posts/20250129/index.html",
    "title": "Thoughts While Checking My Language Skills",
    "section": "",
    "text": "I had a post idea all lined up and then something new landed in my lap. Well, more accurately, it landed in my inbox. I have been tasked with providing some aggregates and means for certain service calls. When I’m looking through some collected data to give me the baseline, I realized that I have four separate columns in my dataset that could be used as the starting timestamp. Many of the rows in the dataset have identical values in all 4 columns. For the rows that don’t, there can be differences between columns of up to 45 seconds. When I asked the requester if there was a preference for which column to use and I received explanations about how further research should be left for operations. I’m ok with that. Ultimately, I was able to learn that there was no preference and I am free to select the starting point I wish.\nHowever, it did get me thinking. I want to know if there is a specific timestamp that should be used as the starting point? Can that be determined statistically? I know that I can create SQL to isolate the earliest time point when there is a discrepancy. However, this could be an interesting project. So, I think I’m going to design it and work on it a bit."
  },
  {
    "objectID": "posts/20250106/index.html",
    "href": "posts/20250106/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "With the snow storm that moved from the midwest over the Appalachians and through Virginia, I received texts and a phone call from a friend and former colleague. She asked if I knew of a way to view all of the current calls for service throughout the state. No, she’s not crazy, she was investigating a request made of her. However, the commiunication started me thinking about an “end-to-end” data science solution that could make that a reality.\nThwew are severak steps that need to be taken to create such an application. Granted, the end result would be focused"
  },
  {
    "objectID": "posts/20241223/index.html",
    "href": "posts/20241223/index.html",
    "title": "Thanks and Thoughts",
    "section": "",
    "text": "Well, the site is now live, thanks to Ionos’ technical support. I have to compliment the person who followed up with me today. He helped me get everything taken care of and get the site live and available for you to see. It’s nice to go from working to build it to seeing it out there where everyone else can see it and we can go on this journey together.\nI also want to thank Beartiz Milz for writing an awesome tutorial that helped me turn my limited knowledge of Quarto and my comfort with R into a website and blog to promote my work in 911 data science. I’m also using her tutorial to build a blog as a vehicle for new reports in my PSAP.\nFor the last 14 years, I’ve written reports for two different PSAPs and felt like my reports have not illuminated everything that the data has to offer. The weekly and monthly reports have been things I’ve been asked to develop. However, there is always so much more in the data which could be important. Can we use knowledge about what days and hours are busiest change how we schedule telecommunicators? Can we see training oportunities by focusing on how long it takes us to process specific call types? Are there hidden correlations in the data that we’ve been missing because we don’t see them? I’ve thought of these questions forever. Now that I’ve finished my doctorate, I want to spend the time investigating these and other questions.\nI think that I can build templates that I can share with other interested PSAPs and we can start building better reporting systems which can, by being served as dashboards and websites, make the reports more responsive while automating the delivery. I want to mke things easier for everyone.\nI updated my Medium site with an annoucement that this blog exists and has become active. I’ve also updated LinkedIn with a link to that article and invited my friends and contacts there to add comments and ask questions that can turn into blog posts. I will post this article there too, once I’ve published it.\nSo, this is where this post ends. I will post something new in a couple of days. I’m going to challenge myself to add some code in the next one to see what happens. I have a couple of ideas in mind."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Dr. D Public Safety Data Science",
    "section": "",
    "text": "NENA 25 National Convention - Long Beach\n\n\n\nupdates\n\nnews\n\n\n\n\n\n\n\n\n\nJul 4, 2025\n\n1 min\n\n\n\n\n\n\nHow to Start - Asking the First Question\n\n\n\nexamples\n\nanalyses\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n17 min\n\n\n\n\n\n\nUpdated Directions\n\n\n\nnews\n\nthoughts\n\nupdates\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n4 min\n\n\n\n\n\n\nDirections for Future Posts\n\n\n\nthoughts\n\nideas\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n1 min\n\n\n\n\n\n\nIt’s About Time\n\n\n\nnews\n\neditorial\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n1 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\neditorial\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n4 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\nfull-stack\n\ndata science\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n1 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\nanalytics\n\n911\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n1 min\n\n\n\n\n\n\nThanks and Thoughts\n\n\n\nnews\n\nthoughts\n\nreports\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n3 min\n\n\n\n\n\n\nThe Start of a New Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n3 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am Dr. Tony Dunsworth. I have been working with data in public safety answering points (PSAPs) for nearly 15 years. Over that time, I work to find new ways to use the data generated by PSAPs to help a center be better staffed, more responsive to the community, and to help the community discover issues that can be addressed to reduce calls to the center. During my tenure in public safety, I’ve worked as a CAD manager, DBA, Data Analyst, and a Data Scientist.\nI completed my PhD work at National University in Data Science. Most of my research concerned both Public Safety Emergency Communications Centers 911/999/112 systems and time series forecasting models. My dissertation evaluated the efficacy of automated forecasting libraries like Prophet or TimeGPT in building accurate forecasts for noisy multi-stochastic datasets like Emergency Communication Center call volumes."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nNational University | La Jolla, CA PhD in Data Science | July 2021 - October 2024\nWestern Governors University | Salt Lake City, UT M.S. in Data Analytics | September 2017 - May 2021\nUniversity of Phoenix | Kansas City, MO Masters in Information Systems | February 2005 - May 2007\nWestern Governors University | Salt Lake City, UT B.S in Software Development | September 2012 - June 2017\nUniverity of Kansas | Lawrence, KS B.A. in Linguistics | August 1987 - May 1991"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nCity of Alexandria | Database Administrator II | December 2014 - present\nJohnson County Sheriff’s Office | CAD Administrator | Sept 2010 - November 2014"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr. D’s Data Science",
    "section": "",
    "text": "I am a data scientist focusing on the use of data to improve emergency operations in our communities. I also research zero-shot time series forecasting libraries. I want to develop automated analytical libraries to be used in 911/999/112 centers to give staff access to better intelligence through the application of advanced techniques."
  },
  {
    "objectID": "posts/20250105/index.html",
    "href": "posts/20250105/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "So, over the dead week between Christmas and New Year’s Day, I started working on a new reporting template. The goal is to create a new archive of weekly and monthly reports for my 911 centre in a Quarto blog format. Currently, I’m working on a sample report for management to see what the audience will like. I also want to use the sample to illustrate what needs to happen with the reports.\nAn example of this is in our datasets, we trap the time stamps at various points in the life of the service call. Some of these include the start of the call, when it was available to be dispatched, when the first units were assigned, and when the call was disconnected. In the data cleaning process, we have identified some of the deltas between time stamps come out negative. Obviously, you cannot dispatch a call before it enters the queue. This means that when we find them, we have to address them in some fashion."
  },
  {
    "objectID": "posts/20250128/index.html",
    "href": "posts/20250128/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "I just finished a week at NENA’s Standard and Best Practices. I had a wonderful time with my presentations on developing staffing models, data management, and using data better in call processing. I met a lot of new friends and I see a lot of opportunities ahead of me to make a real difference at the intersection of public safety and data science.\n\n\nI participated in seven presentations. I will upload slide decks and notes later for people who are interested in learning what I was talking about in detail.\n\n\nThree of these were in a block discussing the need for a new staffing model recommendation in 911 centres. Currently, NENA’s Staffing Recommendations are over 21 years old and rely heavily on the Erlang-C model for staffing recommendations. There has been a tonne of work in queuing theory since the development of this formula. And, as referenced in the linked paper, there are legitimate questions about how appropriate the Erlang model family is for regular call centres, let alone 911 centres.\nOne of the central problems is that Erlang-C models assume there is only one queue that delivers all of the customers to the servers. In most modern centres, there are at least 4 ingression points to the server set. We have dedicated 9-1-1 trunk lines, typically most primary PSAPs also answer some non-emergency lines, and most centres also take 9-1-1 SMS messages. Finally, all centres have to be able to receive and process TTD-TTY calls. Many larger centers may also process traffic cameras, ASAP2PSAP generated alarm calls, or other monitoring services like ShotSpotter. Dispatchers also have radio traffic from the units that they monitor which require their attention. So, there are numerous queues to access the servers, the servers are all multi-skilled servers, and the different queues have different priorities for servers depending on skill sets and responsibilities. Each of these adds levels of complexity that could confound an Erlang-C model.\nThere are several different methods that could come into play, including Markovian models, Game Theory, and Priority Queuing Models. Each of these can be explored and addressed.\nI remain convinced that we have enough brain power in NENA that our working group can tackle this issue and come up with a queuing model that will work for the industry.\n\n\n\nTwo of the other presentations centered around the Emergency Incident Data Object. This is how, we propose, to move information between different systems and entities in public safety. It’s in a JSON object that leverages NIEM to define the objects passed between entities. My presentations dicussed the JSON object itself and the conveyance mechanisms by which the JSON objects are moved between entities.\nI believe in the use of the EIDO as we continue to roll out NG9-1-1 services across the country. We have done a lot of good work throughout the years to create something that can be useful in ensuring data gets to where it needs to go and can be processed efficiently.\n\n\n\nThe other presentations in which I participated were the Call Processing working group where I am the ‘data SME’ and the Future Think committee’s Data Management and Presentation work. That was a fun panel discussion with a lot of great audience participation. There is a general agreement that we need to use more of our data and use it more effectively.\nI am heartened, as a data scientist working in public safety, to hear my colleagues in public safety acknowledging the need to use data analytics, regression modeling, predictive analytics, and forecasting to improve things for telecommunicators.\nThere is also a movement to start considering a shared data repository for research and analytical purposes. I beieve that will be an excellent idea. If we can get several PSAPs together and build out a research database, we could put it in front of other researchers and see where it leades us.\n\n\nI changed the link for NIEM to go to the About page to give readers a better explanaiton of what NIEM is and why it’s important. I also linked to a page describing JSON in better terms. Finally, my editor found a spelling error that was also corrected."
  },
  {
    "objectID": "posts/20250128/index.html#nenas-sbp",
    "href": "posts/20250128/index.html#nenas-sbp",
    "title": "So where does the data take me?",
    "section": "",
    "text": "I participated in seven presentations. I will upload slide decks and notes later for people who are interested in learning what I was talking about in detail.\n\n\nThree of these were in a block discussing the need for a new staffing model recommendation in 911 centres. Currently, NENA’s Staffing Recommendations are over 21 years old and rely heavily on the Erlang-C model for staffing recommendations. There has been a tonne of work in queuing theory since the development of this formula. And, as referenced in the linked paper, there are legitimate questions about how appropriate the Erlang model family is for regular call centres, let alone 911 centres.\nOne of the central problems is that Erlang-C models assume there is only one queue that delivers all of the customers to the servers. In most modern centres, there are at least 4 ingression points to the server set. We have dedicated 9-1-1 trunk lines, typically most primary PSAPs also answer some non-emergency lines, and most centres also take 9-1-1 SMS messages. Finally, all centres have to be able to receive and process TTD-TTY calls. Many larger centers may also process traffic cameras, ASAP2PSAP generated alarm calls, or other monitoring services like ShotSpotter. Dispatchers also have radio traffic from the units that they monitor which require their attention. So, there are numerous queues to access the servers, the servers are all multi-skilled servers, and the different queues have different priorities for servers depending on skill sets and responsibilities. Each of these adds levels of complexity that could confound an Erlang-C model.\nThere are several different methods that could come into play, including Markovian models, Game Theory, and Priority Queuing Models. Each of these can be explored and addressed.\nI remain convinced that we have enough brain power in NENA that our working group can tackle this issue and come up with a queuing model that will work for the industry.\n\n\n\nTwo of the other presentations centered around the Emergency Incident Data Object. This is how, we propose, to move information between different systems and entities in public safety. It’s in a JSON object that leverages NIEM to define the objects passed between entities. My presentations dicussed the JSON object itself and the conveyance mechanisms by which the JSON objects are moved between entities.\nI believe in the use of the EIDO as we continue to roll out NG9-1-1 services across the country. We have done a lot of good work throughout the years to create something that can be useful in ensuring data gets to where it needs to go and can be processed efficiently.\n\n\n\nThe other presentations in which I participated were the Call Processing working group where I am the ‘data SME’ and the Future Think committee’s Data Management and Presentation work. That was a fun panel discussion with a lot of great audience participation. There is a general agreement that we need to use more of our data and use it more effectively.\nI am heartened, as a data scientist working in public safety, to hear my colleagues in public safety acknowledging the need to use data analytics, regression modeling, predictive analytics, and forecasting to improve things for telecommunicators.\nThere is also a movement to start considering a shared data repository for research and analytical purposes. I beieve that will be an excellent idea. If we can get several PSAPs together and build out a research database, we could put it in front of other researchers and see where it leades us.\n\n\nI changed the link for NIEM to go to the About page to give readers a better explanaiton of what NIEM is and why it’s important. I also linked to a page describing JSON in better terms. Finally, my editor found a spelling error that was also corrected."
  },
  {
    "objectID": "posts/startingpoint/index.html",
    "href": "posts/startingpoint/index.html",
    "title": "The Start of a New Blog",
    "section": "",
    "text": "This is, for me, a starting point post. I am just starting to develop this site, including this blog, as a way to do several things at once. First and foremost, I want to document my work in public safety communications data science. This is a turning point in my career, transitioning from back-end administration of dispatch systems to data analytics and data science.\nAnother part of this blog will discuss things I find interesting in tech, especially when it comes to data. For example, I have been working more and more in Python lately and finding tools that I enjoy using because they make my work easier. The top three of late are uv, DuckDB, and FireDucks. Over time, I will add Jupyter notebooks here so I can document my work with these and other tools.\nI also want to learn more about Julia. So I might experiment with it in notebooks as well here and there and document the experience. However, most of the code that you see here will be in R, because it was the first language that I learned and the one that I feel most comfortable with. That will present its own challenge because many forecasting libraries that I have worked with are only expressed in Python.\nThere will be some tutorials as I write more and I get more comfortable with writing here and updating the blog and the site. Most of those are going to be geared towards public safety communications. I work with NENA on several technical committees and part of that work, for me, focuses on bringing advanced analytics to PSAPs across the country. I also speak a few times a year on this and other topics. I will start blogging about what I have presented after the presentations are done. It will be good review.\nI am also fascinated with generating synthetic data for analyses. Right now, I’ve been working with Mostly.ai to generate synthetic data for my research work. Due to the nature of the work that I do in centres, I use synthetic data for my presentations and my tutorials. My goal is to work with other libraries to create more realistic data for research purposes. I know that I will write about that in future posts. I’m looking forward to having fun with that exploration.\nMany of my posts will be found on my Medium page. Some of the tutorials may also be found, in the future, on my LinkedIn. However, I will point my audience here over time because there will be posts here that don’t exist other places so I can promote this place as my prefered publication venue.\nWell, I think that this is a good start as a pre-launch blog post."
  },
  {
    "objectID": "posts/20250406/index.html",
    "href": "posts/20250406/index.html",
    "title": "Updated Directions",
    "section": "",
    "text": "It has certainly been way too long since I’ve written a blog post. I know that I promised myself and my readers, if I have any left, that I would do better with this, but here we are. Life has a funny way of getting in the way when you least expect it. However, those same recent events have given me considerable fodder for new posts that could be in the pipeline.\nI’ve been working on a few projects lately. Many of those projects are going to be the bases of many future posts. The first of those is a piece of software that I’m working on`to generate more realistic synthetic data for use in presentations and data science for public safety. The problem with the current iteration of the software is that I don’t like how the distributions are appearing. I’m working on different distributions to create one that I believe will be more representative of actual centre data.\nthe other large project on the deck is called It’s About Time! Time is essential in every part of any 9-1-1 service call. It starts with a time stamp at every step in the call process. The passage of time can determine a call’s outcome. We can go further and ask how one event can influence another. Can the time spent at a specific point in the process tell you how likely the event will be successful? Can we see how well we meet our SLAs to partner organizations by viewing how well we handle the setup?\nIn each CAD on the market, calls that have been closed can be re-opened or cloned into a new call. The preferred method is to clone the call to preserve the timestamps in the original call. When a call is re-opened, some of the timestamps associated with it will change. This can create very large values and can create negative values when subsequent events seem to occur prior to the prior values. Both of these situations can throw off a centre’s statistics and alter the perception of how it performs. Now that those skewed values are in place after a re-opened call, subsequent analysis must employ amelioration techniques to address those outlying values.\nI am also involved in a group that is looking to build a new staffing recommendations model for 9-1-1 centres. The organisation with which I volunteer, NENA, is looking to replace a 20+ year old study of staffing models with something that is balanced, appropriate for PSAPs of all sizes and call volumes. We also want to look at other factors that can identify good dispatchers and then find better ways to keep them.\nI have been working, for some time, on creating a new series of reports for the centre I work in. However, I think I want to extend that another level. I think that I would like to create software that would allow a centre to select a csv file, pass it like a parameter to the software and get an analysis in return. I think that I would want to make a stand-alone version and a cloud version. The cloud version would be less expensive in return for use of the data, after anonymization, for additional research projects. I will start building my lists of must-have, want-to-haves, and would-be-nice-to-haves. From there, I will design everything from the ground up and then we will see where it goes from there.\nSo, dear readers, if you have any comments, questions, thoughts, or cards to the players; feel free to let me know. I would like to hear from you and see where we can go next."
  },
  {
    "objectID": "posts/20250204/index.html",
    "href": "posts/20250204/index.html",
    "title": "Directions for Future Posts",
    "section": "",
    "text": "I’ve been considering the"
  },
  {
    "objectID": "posts/20250415/index.html",
    "href": "posts/20250415/index.html",
    "title": "How to Start - Asking the First Question",
    "section": "",
    "text": "I recently wrote about a presentation that I gave at Randolph Macon College concerning using data analyses in 9-1-1 centres. During the Q&A section, someone asked me how I would recommend getting started. My answer, then and now, is pick a question and dive into that and new questions will start coming.\nI thought, perhaps, I should come up with an example of what I mean. In our centre, our medical director has requested that we do quality checks on every cardiac arrest call that we receive. So, here is the starting question: what can we learn about the cardiac arrest calls in the city? With that as the opening question, the first step is collecting the data. To start, I plan on collecting four datasets. I can create all of them using SQL. Since I work on SQL Server or T-SQL flavoured databases, the query, for our dispatch software’s databases looks something like this:\n\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 = '2025-01-01 00:00:00.0000000';\n\n-- This query will retrieve all cardiac arrest calls from this year.\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n\n-- This query will retrieve  all cardiac arrest calls from the past 1, 3, & 5 years\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date BETWEEN DATEADD(YEAR, -1, @time1) AND @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n\nThese queries will generate the four datasets that I would want for the full analysis. I would save the output to csv files and name them cardiac_arrest_cy.csv, cardiac_arrest_1y.csv, cardiac_arrest_3y.csv, and cardiac_arrest_5y.csv.\nPersonally, I want to start with current data so I can get a feel for the data. To do some of my work, I would add some columns to the dataset. I can do it programmatically or through the SQL Query. I prefer to do it in the query like so:\n\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 '2025-01-01';\n\n-- This query will retrieve all cardiac arrest calls from this year\n\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n    DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n    DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n    DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\n  AND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n\nThis gives us a columns of elapsed times to determine how long it took us to make the call dispatchable, how long it took to dispatch the call to the first unit, and how long we spent on the phone with the caller.\nNow we load the dataset, for this I’m using the R programming language. We can do it in Python as well, but I’ve been working with R a lot longer.\n\ndf_cacy &lt;- read.csv(\"cardiac_arrest_cy.csv\", header = TRUE, sep = \",\", stringsAsFactors = TRUE)\n\nNow that we have the dataset loaded, we can go through the dataset to clean it up. Most of these calls should have all of the components that we’ve selected. If there are things missing, then we can go in and clean those up to remove missing data points. For this dataset, this is the code I would use to clean up any missing values:\n\n# Check the data\nstr(df_cacy)\nnrow(df_cacy)\nhead(df_cacy, n = 10)\ncolnames(df_cacy)\nspec(df_cacy)\n\n# Use the naniar package to check for missing values. This creates a graphical view of the missing data\ngg_miss_var(df_cacy)\n\n# Use this code to create a quick table of those missing value counts\napply(X = is.na(df_cacy), MARGIN = 2, FUN = sum)\n\n# This code replaces missing values with an entry\ndf_cacy$Call_ID &lt;- tidyr::replace_na(df_cacy$Call_ID, \"NOT RECORDED\")\ndf_cacy$Start_Time &lt;- tidyr::replace_na(df_cacy$Start_Time, \"1970-01-01 00:00:00\") # This is the Unix Time start value\ndf_cacy$Address &lt;- tidyr::replace_na(df_cacy$Address, \"NOT RECORDED\")\ndf_cacy$Latitude &lt;- tidyr::replace_na(df_cacy$Latitude, 388048)\ndf_cacy$Longitude &lt;- tidyr::replace_na(df_cacy$Longitude, 770469)\ndf_cacy$Ready_To_Dispatch &lt;- tidyr::replace_na(df_cacy$Ready_To_Dispatch, \"1970-01-01 00:00:00\")\ndf_cacy$First_Unit_Assigned &lt;- tidyr::replace_na(df_cacy$First_Unit_Assigned, \"1970-01-01 00:00:00\")\ndf_cacy$Stop_Time &lt;- tidyr::replace_na(df_cacy$Stop_Time, \"1970-01-01 00:00:00\")\ndf_cacy$Call_Entry_Time &lt;- tidyr::replace_na(df_cacy$Call_Entry_Time, -1) # This makes any elapsed time that is missing a value we can eliminate in the next cleaning step.\ndf_cacy$Call_Queue_Time &lt;- tidyr::replace_na(df_cacy$Call_Queue_Time, -1)\ndf_cacy$Call_Processing_Time &lt;- tidyr::replace_na(df_cacy$Call_Processing_Time, -1)\n\nThis should clean the data of any missing values. This is also the start of new questions. How much data is missing? Where is it missing at? Finally, why is the data missing? Most of the data is likely not missing at random. I expect that most of the missing data comes from calls that were not completed. When the other three benchmark datasets are compared, we can see how the percentage of calls missing data compares over 1, 3, and 5 years. If the comparisons are in line, then you move on, if they aren’t, there’s question number 4, what’s different now?\nThe next thing that I check for would be negative values in the final three columns, Call_Entry_Time, Call_Queue_Time, and Call_Processing_Time. These varables represent the elapsed times between events in the centre that measure our telecommunicators actions. If we see any values in these variables that are negative, then we know, instantly, that the calls have been closed then reopened. The fifth questions becomes why are some calls being reopened? My recommendation is to take those calls, and the calls with missing values if they can be proven to be full calls, and create a new dataset. That list can be exported to an Excel spreadsheet and used for future investigation. This is the first potential project that has been generated by this work.\nDepending on the number of calls that exist in this dataset, I would remove the same calls from my master datasets and continue with the research without them. My benchmark is no more than 5 percent of the calls removed. If it appears that we’re going to go over 5 percent, we have another question, why are there so many calls with these defects?\nIn a quick glance after running the queries, I found that none of my entries for this year had any missing data and that historical data had missing data in the Time_First_Unit_Assigned column. These would be calls that were received, but never ran for whatever reason. This is what I expected to see in real data.\nOnce the call list has been cleaned, the next step that I recommend is to look at individual variables. I prefer to start with the numeric variables and create a summary. I have a custom summary that I use for my analyses. I feel like it serves my needs and gives me some insights that I will confirm when I create visuals for others. That summary can be found here:\n\n# Custom summary function\ncustom_summary &lt;- function(column) {\n  c(\n    Minimum = round(min(column, na.rm = TRUE), 2),\n    Mean = round(mean(column, na.rm = TRUE), 2),\n    Median = round(median(column, na.rm = TRUE), 2),\n    Q1 = round(quantile(column, 0.25, na.rm = TRUE), 2),\n    Q3 = round(quantile(column, 0.75, na.rm = TRUE), 2),\n    P90th = round(quantile(column, 0.90. na.rm = TRUE), 2),\n    Maximum = round(max(column, na.rm = TRUE), 2),\n    Std_Dev = round(sd(column, na.rm = TRUE), 2),\n    Variance = round(var(column, na.rm = TRUE), 2),\n    Skewness = round(e1071::skewness(column, na.rm = TRUE), 2),\n    Kurtosis = round(e1071::kurtosis(column, na.rm = TRUE), 2)\n  )\n}\n\nThis custom summary gives me a very good picture of the data prior to creating any visualizations. The minimum and maximum values give me a good range for the data, while the mean and median paint a picture of the central tendencies of the data. The standard deviation and variance can give me some information about the overall spread of the data, the skewness and kurtosis values give me the shape of the distribution, and the Q1 and Q3 values suggest the boundaries for outlier identification. All of this together now gives me a good numeric ‘visualization’ of the data and now I can focus on asking new questions of the numeric data.\nAdditionally, I will want to break the Response_Date column into some constituent components such as the day of the week, the day of the year, the hour, and the week number. These will allow me to create different pictures. To get those, I typically generate them from the SQL Query. That updated query would look like this:\n\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    CAST(DATEPART(WEEK, Response_Date) AS NVARCHAR(2)) AS [WeekNo],\n    UPPER(FORMAT(Response_Date, 'ddd')) AS [DOW],\n    CAST(DATEPART(DAY, Response_Date) AS NVARCHAR(2)) AS [Day],\n    CAST(DATEPART(Hour, Response_Date) AS NVARCHAR(2)) AS [Hour],\n    Address,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n  DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n  DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n  DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\nAND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n\nor you can do it programmatically in R like this:\n\n# Function to extract day of week (DDD), week number, year, and hour\n# This also uses the lubridate library\nextract_datetime_features &lt;- function(df_cacy, Response_Date) {\n  df %&gt;%\n    mutate(\n      day_of_week_ddd = wday(!!sym(Response_Date), label = TRUE),\n      week_number = isoweek(!!sym(Response_Date)),\n      year_number = year(!!sym(Response_Date)),\n      hour_of_day = hour(!!sym(Response_Date))\n    )\n}\n\nEither way, thsis allows us new data to generate new questions and answers. Is there a specific day of the week that has more events than others? Is there a certain time of the year that has more events? Are there specific hours out of the day that see more events? These questions could impact staffing, training, and could present opportunities to partner with the local hospitals to create better synergies for positive patient experiences and outcomes. So we’re now at 8 questions from our starting point as well as a couple of different projects. And when you take a look at the summaries for each of the elapsed times and compare them to the 1, 3, and 5 year summaries, you can not only see trends but find new questions when doing the comparisons to those benchmarks.\nFrom here, for the more visually minded, we can create two or three visualizations for each of the three elapsed time columns. I recommend a histogram or density plot, a blox plot, and a QQ plot. The last one will show you, visually, what the skewness and kurtosis told you numerically. This code should create those for you. Just remember that you’re doing it for each of the three variables.\n\n# Historgram and Density plt\nggplot(df_cacy, aes(x = Call_Entry_Time)) + \n  geom_histogram(aes(y = ..density..),\n                 colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)\n\n# QQ Plot\nggqqplot(df_cacy, x = \"Call_Entry_Time\", color = \"#1c5789\", title=\"QQ Plot of TimeToQueue\", ylab = \"Call Entry Time\")\n\n## Box Plot\nbox1 &lt;- df_cacy %&gt;% ggplot(aes(x=DOW, y=Call_Entry_Time)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Boxplot of Call Entry Times by Day\")\n\nbox1\n\nComparing the results of the current year to the benchmarks gives you visual verification of what you’ve seen numerically. These are also great for the subsequent reports because your audience is now able to visualize what you’re telling them. This may lead to additional questions about the internal working of the data. This could add to the question and project totals that you have generated.\nVisualizations are not simply for numeric data. We can create boxplots for the factor or categorical data. For example, this allows us to look at each day of the week and see how many cardiac arrest calls we have for each day of the week.\n\ndf_cacy$DOW &lt;- factor(df_cacy$DOW, levels = c(\"SUN\",\"MON\",\"TUE\",\"WED\",\"THU\",\"FRI\",\"SAT\"))\n\n# ggplot2\nbarDOW &lt;- df_cacy %&gt;% ggplot(aes(x=DOW, fill=DOW)) +\n  geom_bar() +\n  scale_fill_viridis(discrete = TRUE, option = \"H\") +\n  ggtitle(\"Count of calls per day of the week\") +\n  geom_text(\n    stat = 'count', \n    aes(label = after_stat(count)), \n    vjust = -0.5  # Adjusts the vertical position of the count\n  )\n\nbarDOW\n\nIf anything stands out in this, we can also check that against the benchmark datasets and determine if this normal or if it’s standing out for a different reason. This can also be generated for each hour of the day with minor changes to the code. This leads us to another plot that can be visually appealing while also giving us some excellent information. We can use a ridgeline plot to give us a count of events per hour per doy. Are there specific times and days which see more events than others. It’s likely that we would see different patters if we examined traffic stops. I expect, if we ran this out, we would not see any particular combination stand out.\n\n# Aggregate data to get counts per hour per day\nhourly_calls &lt;- df_cacy %&gt;%\n  group_by(DOW, Hour) %&gt;%\n  dplyr::tally(name = \"CallCount\")\n\n# Create the Ridgeline Plot\nridge_plot &lt;- ggplot(hourly_calls, aes(x = Hour, y = DOW, height = CallCount, group = DOW, fill = DOW)) + # Add fill aesthetic\n  geom_density_ridges(stat = \"identity\", scale = 0.9, rel_min_height = 0.01) +\n    scale_x_continuous(breaks = 0:23) + # Set breaks for each hour\n  scale_y_discrete(limits = rev) + # Reverse order of days for better readability\n  scale_fill_viridis(discrete = TRUE, option = \"H\") + # Use viridis scale\n  labs(title = \"Call Count by Hour and Day of Week\",\n       x = \"Hour of Day\",\n       y = \"Day of Week\",\n       fill = \"Day of Week\") + # Add legend title\n  theme_ridges(font_size = 20, grid = TRUE)\n\nridge_plot\n\nAnother thing we can do is look at the geographic distribution of cardiac arrests in the city. Where do we have more of them? that might help us position our resources better and assist us with patient care by ensuring that people at those locations are properly trained in CPR to help get hands on chest faster which increases the likelihood of survival. This code will create and display a map of the City with the locations marked. Hovering over any individual data point will also give us call information so we can see other details.\n\n# Convert integer coordinates to decimal degrees\n# For latitude: Divide by 1,000,000 and keep positive (Northern hemisphere)\n# For longitude: Divide by 1,000,000 and make negative (Western hemisphere for Alexandria)\ncalls_with_coords &lt;- df_cacy %&gt;%\n  mutate(\n    lat = Latitude / 1000000,\n    long= -1 * (Longitude / 1000000)  # Negative for western hemisphere\n  )\n\n# Create the map\nalexandria_map &lt;- leaflet(calls_with_coords) %&gt;%\n  # Add base map tiles\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  # Set the view to center on Alexandria, VA\n  setView(lng = -77.0469, lat = 38.8048, zoom = 13) %&gt;%\n  # Add markers for each call\n  addCircleMarkers(\n    ~long, ~lat,\n    color = '#1c5789',\n    radius = 6,\n    stroke = FALSE,\n    fillOpacity = 0.7,\n    popup = ~paste(\"&lt;b&gt;Call ID:&lt;/b&gt;\", Master_Incident_Number, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Date:&lt;/b&gt;\", Response_Date, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Address:&lt;/b&gt;\", Address, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To Queue:&lt;/b&gt;\", Call_Entry_Time, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To DispatchL&lt;/b&gt;\", Call_Queue_Time, \"&lt;b&gt;\",\n                   \"&lt;b&gt;Seconds On Call:&lt;/b&gt;\", Call_Process_Time, \"&lt;br&gt;\")\n  ) %&gt;%\n  # Add a title\n  addControl(\n    html = \"&lt;h3&gt;Cardiac Arrest Calls&lt;br&gt;Alexandria, VA&lt;/h3&gt;\",\n    position = \"topright\"\n  )\n\n# Display the map\nalexandria_map\n\n# Save the map for inclusion in reports\nsaveWidget(alexandria_map, \"alexandria_calls_map.html\", selfcontained = TRUE)\n\n# Optional: If you want to analyze call density by neighborhood or district\n# This creates a clustered view that can help identify hotspots\ncluster_map &lt;- leaflet(calls_with_coords) %&gt;%\n  # Add base map tiles\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  # Set the view to center on Alexandria, VA\n  setView(lng = -77.0469, lat = 38.8048, zoom = 13) %&gt;%\n  # Add clustered markers\n  addMarkers(\n    ~longitude, ~latitude,\n    popup = ~paste(\"&lt;b&gt;Call ID:&lt;/b&gt;\", Master_Incident_Number, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Date:&lt;/b&gt;\", Response_Date, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Address:&lt;/b&gt;\", Address, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To Queue:&lt;/b&gt;\", Call_Entry_Time, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To DispatchL&lt;/b&gt;\", Call_Queue_Time, \"&lt;b&gt;\",\n                   \"&lt;b&gt;Seconds On Call:&lt;/b&gt;\", Call_Process_Time, \"&lt;br&gt;\"),\n    clusterOptions = markerClusterOptions(),\n    label = ~as.character(Master_Incident_Number)\n  ) %&gt;%\n  # Add a title\n  addControl(\n    html = \"&lt;h3&gt;Clustered Call Locations&lt;br&gt;Alexandria, VA&lt;/h3&gt;\",\n    position = \"topright\"\n  )\n\n# Uncomment to save the cluster map\n# saveWidget(cluster_map, \"alexandria_calls_cluster_map.html\", selfcontained = TRUE)\n\nWe have set this up to show us where all of the calls are and when we hover over an individual call, it will give us information about the call including the master incident number, the address, the date and time of when it was generated, and the amount of time that passed before it went to queue, was dispatcahed, and how long we were on the call. Seeing the geographic clusters will likely identify nursing homes or other care facilities, so if we see clusters in other places, we may want to look into why a cluster of cardiac arrests are there. These clusters can also be compared against the benchmarks to see if they look the same. If they don’t, we can start asking why we have a new cluster. That increases the number of questions that we can ask and the number of projects we have in front of us.\nA final example of a new research project would be to create a binary variable in hand with deeper research to determine if calls were originally coded as cardiac arrests or if they were coded and dispatched as something else and changed to cardiac arrests later in the process. That examination could show how accurate we were in triaging and handling these calls. I will leave that code for another day and another post.\nFrom here, you can go anywhere you want to and find answers to the questions that you’ve found. I hope that this showed exactly how you can start with one question, what can we learn about cardiac arrests, and turn it into many questions and projects that show how much is left to know."
  },
  {
    "objectID": "posts/20250704/index.html",
    "href": "posts/20250704/index.html",
    "title": "NENA 25 National Convention - Long Beach",
    "section": "",
    "text": "Once again, I haven’t written in a long time. I know Bad Dr. D!"
  },
  {
    "objectID": "posts/20250801/index.html",
    "href": "posts/20250801/index.html",
    "title": "You never know what is interesting",
    "section": "",
    "text": "About a month or so back, I had an opportunity to speak with several people from a company that makes software for public safety entities. I had connected with one of their folks through my BlueSky account. After a few preliminary conversations, they wanted to read my dissertation. Trust me folks, when someone asks to read it, you are truly excited. I sent them a copy of it and when it came time to meet with the group, I was excited to learn they really did read it!\nWhen we met, they asked me a few questions about it. Overall, they appeared to be impressed with my work. However, the surprise came when I mentioned a small piece of toy software I have written. Because I give many presentations around the country about statistics and data use,I decided to"
  }
]