[
  {
    "objectID": "test_python_setup.html.html",
    "href": "test_python_setup.html.html",
    "title": "Python Test",
    "section": "",
    "text": "print(\"Python is working in Quarto!\")\nimport pandas as pd\nprint(f\"Pandas version: {pd.__version__}\")\n\nPython is working in Quarto!\nPandas version: 2.3.1"
  },
  {
    "objectID": "SETUP_COMPLETE.html",
    "href": "SETUP_COMPLETE.html",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "Your Quarto site now supports both R and Python code execution using uv for fast Python package management.\n\n\n\n\n\nâœ… Virtual environment created with uv venv\nâœ… All data science packages installed:\n\nCore: pandas, numpy, matplotlib, seaborn, scipy\nML/Stats: statsmodels, scikit-learn\nDatabase: sqlalchemy, pyodbc, psycopg2-binary\nGeospatial: folium, geopandas\n\nVisualization: plotly, altair\nJupyter: jupyter, ipykernel\nUtilities: python-dotenv, requests\n\n\n\n\n\n\nâœ… Updated _quarto.yml to support multiple engines\nâœ… Python code execution verified and working\nâœ… Both R and Python can be used in the same site\n\n\n\n\n\nâœ… setup_python.py - Automated setup and testing\nâœ… with_python_env.sh - Convenience script for running commands\nâœ… python_utils.py - Python equivalents of R functions\n\n\n\n\n\n\n\n# Option 1: Activate environment manually\nsource .venv/bin/activate && quarto render\n\n# Option 2: Use convenience script\n./with_python_env.sh quarto render\n\n\n\n./with_python_env.sh quarto preview\n\n\n\n\nCreate Python code blocks in your .qmd files:\n#| echo: true\n#| eval: true\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Your Python analysis code\ndata = pd.read_csv('your_data.csv')\ndata.plot()\nplt.show()\n\n\n\nThe python_utils.py file provides Python versions of your R analysis functions:\n\ncustom_summary() - Comprehensive statistics\nextract_datetime_features() - Date/time processing\n\ncreate_response_time_plots() - Analysis visualizations\ncreate_geographic_map() - Interactive mapping with Folium\nload_sql_data() - Database connectivity\n\n\n\n\nfrom python_utils import custom_summary, create_geographic_map\nimport pandas as pd\n\n# Load your cardiac arrest data\ndf = pd.read_csv('cardiac_arrests_cy.csv')\n\n# Get comprehensive statistics (equivalent to your R function)\nstats = custom_summary(df['call_entry_time'])\nprint(stats)\n\n# Create interactive map (equivalent to your R leaflet maps)\nmap_obj = create_geographic_map(df)\nmap_obj.save('cardiac_arrests_map.html')\n\n\n\n\n_quarto.yml - Added Python engine support\n.venv/ - Python virtual environment\npython_utils.py - Python utility functions\nsetup_python.py - Setup automation\nwith_python_env.sh - Convenience script\nrequirements.txt - Package dependencies (backup)\nPYTHON_README.md - Detailed documentation\n\nYour site now has the flexibility to use either R or Python (or both!) for your data science content. The Python environment provides equivalent functionality to your R workflow with modern, fast package management via uv."
  },
  {
    "objectID": "SETUP_COMPLETE.html#what-was-set-up",
    "href": "SETUP_COMPLETE.html#what-was-set-up",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "âœ… Virtual environment created with uv venv\nâœ… All data science packages installed:\n\nCore: pandas, numpy, matplotlib, seaborn, scipy\nML/Stats: statsmodels, scikit-learn\nDatabase: sqlalchemy, pyodbc, psycopg2-binary\nGeospatial: folium, geopandas\n\nVisualization: plotly, altair\nJupyter: jupyter, ipykernel\nUtilities: python-dotenv, requests\n\n\n\n\n\n\nâœ… Updated _quarto.yml to support multiple engines\nâœ… Python code execution verified and working\nâœ… Both R and Python can be used in the same site\n\n\n\n\n\nâœ… setup_python.py - Automated setup and testing\nâœ… with_python_env.sh - Convenience script for running commands\nâœ… python_utils.py - Python equivalents of R functions"
  },
  {
    "objectID": "SETUP_COMPLETE.html#quick-start",
    "href": "SETUP_COMPLETE.html#quick-start",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "# Option 1: Activate environment manually\nsource .venv/bin/activate && quarto render\n\n# Option 2: Use convenience script\n./with_python_env.sh quarto render\n\n\n\n./with_python_env.sh quarto preview"
  },
  {
    "objectID": "SETUP_COMPLETE.html#using-python-in-posts",
    "href": "SETUP_COMPLETE.html#using-python-in-posts",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "Create Python code blocks in your .qmd files:\n#| echo: true\n#| eval: true\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Your Python analysis code\ndata = pd.read_csv('your_data.csv')\ndata.plot()\nplt.show()"
  },
  {
    "objectID": "SETUP_COMPLETE.html#python-equivalents-of-your-r-functions",
    "href": "SETUP_COMPLETE.html#python-equivalents-of-your-r-functions",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "The python_utils.py file provides Python versions of your R analysis functions:\n\ncustom_summary() - Comprehensive statistics\nextract_datetime_features() - Date/time processing\n\ncreate_response_time_plots() - Analysis visualizations\ncreate_geographic_map() - Interactive mapping with Folium\nload_sql_data() - Database connectivity"
  },
  {
    "objectID": "SETUP_COMPLETE.html#example-usage",
    "href": "SETUP_COMPLETE.html#example-usage",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "from python_utils import custom_summary, create_geographic_map\nimport pandas as pd\n\n# Load your cardiac arrest data\ndf = pd.read_csv('cardiac_arrests_cy.csv')\n\n# Get comprehensive statistics (equivalent to your R function)\nstats = custom_summary(df['call_entry_time'])\nprint(stats)\n\n# Create interactive map (equivalent to your R leaflet maps)\nmap_obj = create_geographic_map(df)\nmap_obj.save('cardiac_arrests_map.html')"
  },
  {
    "objectID": "SETUP_COMPLETE.html#files-addedmodified",
    "href": "SETUP_COMPLETE.html#files-addedmodified",
    "title": "Python Environment Setup Complete! ðŸŽ‰",
    "section": "",
    "text": "_quarto.yml - Added Python engine support\n.venv/ - Python virtual environment\npython_utils.py - Python utility functions\nsetup_python.py - Setup automation\nwith_python_env.sh - Convenience script\nrequirements.txt - Package dependencies (backup)\nPYTHON_README.md - Detailed documentation\n\nYour site now has the flexibility to use either R or Python (or both!) for your data science content. The Python environment provides equivalent functionality to your R workflow with modern, fast package management via uv."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Resume for Tony Dunsworth Ph.D.\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "posts/Chronomancer/index.html",
    "href": "posts/Chronomancer/index.html",
    "title": "Chronomancer Prestige Class 5e",
    "section": "",
    "text": "First off, yes, this is a much different post than some of the ones that I have previously written. Regular readers will note that I typically write about data and 9-1-1 centers. This is a much different and, hopeful, more fun post. Iâ€™ve played Dungeons & Dragons since I was a pre-teen and have played every version of it from the red box set through 5e. My wife has also played D&D for a little longer than I. We have both had a lot of fun with this over the years. This post was started by a conversation my wife and I had earlier when discussing someone we both know working in security and how he was unaware that I had worked in security years ago when I first graduated from university the first time. During that time, I worked a post at a grocery store distribution center and during my rounds, I carried a large clock that was used to prove that rounds have been completed. It consisted of a clock face with a tape spool inside and a keyhole where a key on the wall was inserted and typed against the tape.\nAs we discussed what I used to do as a security guard, we started laughing about how the clock could have been used as a weapon had I encountered anyone. The thing was heavy enough to crack someoneâ€™s skull with a solid blow. It weighed about 3 almost 4 kilos and could be considered a little heavier than a war hammer. So we started discussing what the damage would look like in D&D terms. That then led to my wife suggesting a subclass of a wizard and I came up with the name of Chronomancer. So here is our version of it to be used in a 5e setting. Yes, we do know that others have thought of this as well. You can use it if you wish, or not\n\n\nThe Chronomancer is a subclass of Wizard. So in this instance, you would have to have a few levels of Wizard and have the ability subclass in your campaign. Weâ€™ve considered this as a prestige class. These would be wizards who are obsessed with time, using it, controlling it, changing it. It could be for any reason, the desire for immortality, time saved them from a bad situation, etc."
  },
  {
    "objectID": "posts/Chronomancer/index.html#background",
    "href": "posts/Chronomancer/index.html#background",
    "title": "Chronomancer Prestige Class 5e",
    "section": "",
    "text": "First off, yes, this is a much different post than some of the ones that I have previously written. Regular readers will note that I typically write about data and 9-1-1 centers. This is a much different and, hopeful, more fun post. Iâ€™ve played Dungeons & Dragons since I was a pre-teen and have played every version of it from the red box set through 5e. My wife has also played D&D for a little longer than I. We have both had a lot of fun with this over the years. This post was started by a conversation my wife and I had earlier when discussing someone we both know working in security and how he was unaware that I had worked in security years ago when I first graduated from university the first time. During that time, I worked a post at a grocery store distribution center and during my rounds, I carried a large clock that was used to prove that rounds have been completed. It consisted of a clock face with a tape spool inside and a keyhole where a key on the wall was inserted and typed against the tape.\nAs we discussed what I used to do as a security guard, we started laughing about how the clock could have been used as a weapon had I encountered anyone. The thing was heavy enough to crack someoneâ€™s skull with a solid blow. It weighed about 3 almost 4 kilos and could be considered a little heavier than a war hammer. So we started discussing what the damage would look like in D&D terms. That then led to my wife suggesting a subclass of a wizard and I came up with the name of Chronomancer. So here is our version of it to be used in a 5e setting. Yes, we do know that others have thought of this as well. You can use it if you wish, or not\n\n\nThe Chronomancer is a subclass of Wizard. So in this instance, you would have to have a few levels of Wizard and have the ability subclass in your campaign. Weâ€™ve considered this as a prestige class. These would be wizards who are obsessed with time, using it, controlling it, changing it. It could be for any reason, the desire for immortality, time saved them from a bad situation, etc."
  },
  {
    "objectID": "posts/20250831/index.html",
    "href": "posts/20250831/index.html",
    "title": "Cleaning in the Query",
    "section": "",
    "text": "When I started actively working in data science, like everyone else, I heard the old maxim that a data scientist spends about 80% of their time cleaning data. Well, I can report that in many cases, thatâ€™s true. Iâ€™m currently facing a lot of that right now in building a new dataset for some new reports and dashboards that Iâ€™m designing. Iâ€™m starting the data cleaning process in the SQL statement. I believe that itâ€™s a lot faster in the end. Once I have the query where I want it, then many of the following steps are pretty easy.\nFor this dataset, I have three tables from which I will pull data. The first is the master incident table. This contains most of the data that I need for my reports. The second is an extension to the first table. I only want one column from that table and, honestly, I donâ€™t know how often I would use it. However, for a few types of analyses, it could be very handy. The other table that I want to use is a log of activities in a call and the details associated with each activity. Most of the data is fairly straightforward. Some of the derived data, however, isnâ€™t as straight forward as I would like. That data consists of the deltas between timestamps. We use those for operational assessment.\nOne of the main reasons that I want to do most of my cleaning in the SQL query itself is that I can control the output and standardize it easier. I am more confident with SQL at this stage because Iâ€™ve been writing SQL for nearly 20 years in some form. Having written a lot of queries and them progressively more complex and detailed, it feel more natural for me. I admit, I believe that every data scientist should be fluent in writing SQL queries of all types. I also believe that cleaning in the query will save me a considerable amount of time when Iâ€™m building my reports and dashboards. That doesnâ€™t mean that there arenâ€™t challenges in cleaning the data before it goes into my dataset. The rest of this post will discuss some of those issues.\nWhen I started writing the first iteration of this query, and Iâ€™m currently on my seventh iteration of it, I started then and now with two columns as my foundations. The first is the incident number. I can designate it an ID field in my dataset. The second is the incident creation datetime. From that, I create a lot of additional columns that allow me to craft different analyses. I split the datetime apart to use the hour, the day of the week, and the week number. I also derive the shift and how deep into the shift the call comes. This will be used in correlative studies that investigate how call processing times change throughout the shift. Do we see a change in call processing times when weâ€™re working around lunch breaks? Do the response times change as we get deeper into the shift? Iâ€™m also trapping the problem type, the discipline of the service call, the way that it was received, the final disposition, and internally, we trap the call taker and dispatcher for each call. This also includes the priority number. All of these things are chosen to enable different analyses to be run to learn more about the week that happened and how operations functioned.\nAlong with all of this information, we trap time stamps for various events in the life of a call. One of the reasons for this is to calculate how long it takes for each event to occur and how different calls compare to the median values. It also allows us to track different parts of the operations process and how things change week-to-week. This can lay the foundation for determining if there are correlations between weekly volumes and call processing times. If I find relationships, then I can also build regression models to forecast processing times based on volume, number of telecommunicators on shift, and other factors. However, this is certainly going to be much farther in the future.\nThe problem is the timestamps that are available and what to do with them and about them. In the master incident table, there are four different timestamps that could be identified for the start of the call. The first is the response date. It is actually the date and time that the software records the call in the database. The second is a column that is listed at the time the clock starts. The third is the date and time that the phone pickup is registered by the software. The final timestamp is when the software records the first keystroke in the call taking screen. All of these time stamps can be within a few seconds of each other. The problem is that there is not one that is consistently earliest. After looking at several different options, because this is a Microsoft SQL Server database and needs to use T-SQL, I will have to use a series of CASE() statements to determine the earliest timestamp. If I were using a different database, I might use LEAST() instead. This is not the only place that I will have to deal with this. We also have three time stamps, between two tables, that could determine the time that the calltaker has collected enough information from the caller to release the call for dispatching. I will have to use the same clause to find the earliest value that identifies when the call was ready for dispatch.\nThe other challenge is that the vendor has built some elapsed time fields in the main table that could be used, but they are all strings. So to make any use of them, we have to convert them into an integer value of seconds to compare them to differences between the timestamps that weâ€™ve identified for each stage of the call. When I first started writing the conversion, I made a mistake and it looked as if these two different values were way off in a couple of areas. The mistake was how I handled decimal seconds. My code acted like the decimal seconds were seconds and the seconds minutes, etc. When I found my error and corrected it, then everything fell into place and in most instances, the difference between the calculated elapsed time and the documented string is 3 seconds or less. For most of the calls where there are larger differences between some values, we find calls that have been reopened.\nIdentifying reopened calls and gauging their impact on our statistics was the reason behind this iteration of this query. I wanted to not only identify which calls were closed once then reopened, but I also wanted to find out how to ameliorate their impact on our reported metrics. This updated query has given me the best of both worlds. I will be able to isolate the reopened calls and analyse them for patterns.\nIn several places, I still have NULL values. In several of my non-datetime fields, I use different strings for several of my categorical variables. If they are integer or calculated fields, I use a value of -9999 to identify the null fields. I didnâ€™t want to set the values to 0 because that is a possible value in several places. Again, this cleaning done in the query is a lot simpler and easier to accomplish. In most cases, I use the ISNULL() function to address possible NULL values. By cleaning here, I donâ€™t have to write as much cleaning code and I get the advantage of moving directly into my analytical code and getting the to the insights hidden in the data.\nWhen Iâ€™m done, I have a dataset that has completed an initial cleaning and is ready for the needed manipulations and transformations that will allow me to find new insights and deliver value. Sure the query to generate these datasets is kinda heavy. This new version clocks in at just under 600 lines when formatted in a way that is more human-readable, but it runs quickly, returns over 1000 rows of data in under 3 seconds. I have a second version of this that isnâ€™t focused on a weekly output. That version allows for the specification of dates so I can create larger datasets.\nSince I also have a synthetic generator that I continue to update and support, I can also use this query to generate datasets that will allow me to gather more accurate data to fine-tune the distributions and make certain that my synthetic data is more useful. I will likely use the disfit library to make more accurate assessments of the data and find the best distribution.\nWell, thatâ€™s it for this edition. Stay tuned for a different swing through the world of data workers coming soon!!"
  },
  {
    "objectID": "posts/20250812/index.html",
    "href": "posts/20250812/index.html",
    "title": "Remembering My Target Audience",
    "section": "",
    "text": "I like soliciting feedback for this blog so I can refine my work and ensure that Iâ€™m reaching my target audience effectively. I received some valuable feedback recently that made me reflect on my target audience and how I communicate with them. I meant for this to appeal to professionals in the 9-1-1 and public safety fields, along with people interested in statistics, analytics, and data science overall. The feedback I received after my most recent post suggested that I needed to reflect on my target audience more carefully.\nMy feedback came from two highly intelligent professionals, both Toastmasters for whom I have deep respect. One is a 9-1-1 professional and the other a retired medical professional. Both of them gave me similar feedback. They both enjoy my writing style and felt that I write well. However, they also pointed out that I lost them in some of the technical details. I went a little too deep into the details. I think thatâ€™s because I havenâ€™t deployed my framework to GitHub yet.\nSo, I want to back my own bus up a bit, so to speak, and recast my last post in a little better format. I think this will reach my target audience more effectively and, hopefully, make the technical choices clearer to my actual target audience.\nWhile you can perform data analysis in many different languages and you can do a fair piece of data analysis in Excel, there are three main choices, programmatically, to do basic, intermediate, and advanced data analysis. These are Python, R, and Julia. R has a reputation for being a great statistical programming language and can be closely associated with academia. Julia is the new kid on the block and has a lot of promise. However, I donâ€™t see as much work daily work in it. If you are interested in Julie, I recommend reading Emma Boudreauâ€™s Medium Blog and following her GitHub Repositories. They give an excellent view of what Julia can do beyond basic data analysis. Python is an excellent general programming language with a lot of supplemental libraries that can speed up its performance and make it an excellent choice for data analysis, data engineering, machine learning modelling, and even Large Language Model (LLM), and AI development. So I chose Python because of the libraries I know that I need as well as ones that I think I will need as the project grows through the phases that Iâ€™ve envisioned for it.\nBecause a typical Python project requires several libraries, there is always a high-likelihood that some of the libraries will not play well together at the most up-to-date version of Python. Currently, the most recent release according to the python.org website is 3.13.13. Many of them will work together at some earlier version, so a key piece of building the infrastructure for the project is version management and control. There are, obviously, many ways to do that. Iâ€™ve tried several different methods and Iâ€™ve settled on one that really works well for me. I use uv by Astral Software. I also use their linting and formatting tool Ruff, but that will be discussed a little bit later. I chose it because, in many cases, you can, after installation, use many of the same pip or venv commands to configure your environment and install libraries and only preface them on the command line with uv. Astral wrote uv in Rust which is known for creating very fast software. This speeds up Pythonâ€™s built-in tools and expands your capabilities by adding commands like {bash} $ uv tool install ruff or {bash} $ uvx ruff to install tools and executing scripts with a command like {bash} $ uv run synth911gen.py.\nOther environments, like Anaconda, provide full environments, but the libraries can lag behind and it is appears to have a lot of overhead. Poetry, like uv, supports greater collaboration since the environment can be packaged in a single toml file and shipped to another workstation. From there, both can use their version of toml to synchronize the environment and download the necessary libraries with all the needed dependencies. I just found uv easier to use and less argumentative than Poetry. Having said that, remember that your milage may vary and if one works better for you than the other, by all means, use what works best for you. I know there may be readers who will ask if you can do most of this by prepending â€˜uvâ€™ in front of other Python commands, why not just use pip and venv? My answer is that it works quickly and seems more consistent to me. While writing this, I actually found a different environment manager that could be interesting since it touts its ability to be multilingual. It is called pixi. I have no experience wiht it, but who knows. I am going to read the docs on it to see if it might be useful for me in the future.\nOnce the environment is set, then I ensure that I install a version of Python for that environment. Right now, Iâ€™ve stayed with the latest patch to 3.11 which is 3.11.13. It has an end of life date in 2027, so I can work with it and I know that it is still in support while I build the project. Most of the libraries that Iâ€™ve chosen are compatible with it and with each other at that version. I have considered moving to 3.12, but I havenâ€™t tested that all of my libraries will work together and with 3.12. As I move forward in the project, I will test version and patch levels to see if everything will work as I would like it to. After the environment is set, I need to start working on the libraries that I want to use in the project. The first library choice is the most fundamental; choosing the library that creates and manages data frames. The most common library is pandas. However, the biggest complaint with this library is speed, or lack thereof. Iâ€™ve used Fireducks in the past with some really good results. I also know that Polars is a very fast data frame library. So, since it is designed to handle large data sets, I have decided to use it for this project. I have noticed a trend lately of libraries being written in Rust to speed up performance. It makes me think about cutting out the middle man and finding a way to use Rust directly for data analysis, but that is a project for another day. For much of the heavy statistical math lifting, I plan on using NumPy along with SciPy and Statsmodels. These three libraries cover most of my statistical and mathematical needs and are standard libraries in the data science world.\nWhen it comes to graphics, while there are tried and true standards like Seaborn, I have always preferred both plotly and plotnine, I prefer using plotnine because it implements the grammar of graphics as outlined in The Grammar of Graphics by Leland Wilkinson. If you are familiar with R, it is similar to ggplot2 which is based on the same principles. Once you become accustomed to itm the Grammar of Graphics is easy to understand. This Toards Data Science post explains the principles very nicely, including a good representation of the multiple levels starting with the data and moving through to the coordinate system. It makes a very effective way to build your visualizations and it allows for the reuse of many components by allowing for minor changes to the aesthetics or the geometries. Geometries or geoms are how you display data points. Are you using bars, points, lines, xâ€™s, etc.\nFinally, other libraries for all phases of the project include pydantic for type safety, quarto for publishing documents, and Jupyter for notebooks that allow code testing. Python is not a type safe language. You can assign any value to a variable and it will infer the type; string, integer, boolean, etc from the value assigned to it. You can change the variable value which can change the type and Python will happily try to apply the variable to the function. As an example, you can creats a variable x and assign the value 1 to it. Now, you can add 1 to it and save the output as y. If you print y, you should get 2. Python inferred that 1 is an integer and added it to another integer and came up with an integer result. However, you can then change x to â€˜1â€™ and run the same code and youâ€™ll get â€˜11â€™. Python saw a string, cast the second 1 to a string and concatenated the two strings together. You can specify, in a function definition, that the input value for x has to be an integer and it will throw an error if it finds a string. However, when youâ€™re working fast, you might forget to define your types in the function. Pydantic takes care of that by reading the variable and watching the use and flagging where the usage may be questionable and helps ensure what you program is what you meant to write. Quarto is a great document publishing system that can use R, Python, Julia, or even Javascript to execute code in a document and display the results. You can even display the code that generates the output. In fact, this website and blog use Quarto as the foundation. Eventually, I can get fancy with CSS and Javascript to make it even prettier. You can use this platform to create many different types of output. You can use pandoc to create Microsoft Word compatible documents, you can use either LaTeX or typst to create pdfs or publication-ready documents formatted to a specific journals requirements. You can use it with Shiny to create interactive dashboards, and you can create PowerPoint slides with it. Jupyter, on the other hand, is good for creating â€˜notebooksâ€™ that can be shared and reproduced to show how code is working. I prefer to use it like a scratch pad when Iâ€™m creating code for testing purposes. After Iâ€™m satisfied, I can take the code and move it over to a document. However, using quarto, I could turn a jupyter notebook into a document with ease. Again, it all depends on what you want to do and how you want to do it.\nPhase I of this project is really this: building the infrastructure. After the infrastructure is built, then all of the other phases can flow into it to ensure that everything gets completed when each step is ready to deploy. Phase II, after getting the infrastructure built, is to create a base document that will serve as the model for future reports. Initially, the report will come out weekly and will cover summaries of different data slices. My goal is to create a report that could almost mimic a newsletter. Here are the highlights and the standard KPI reports. That will expand after a time to include a comparison with the week prior, then 4 weeks prior, then 8 weeks prior and at the 4 and 8 wees, a comparison with the same week number for the previous year will be included. Trends and historical information are both important to ensure that centre operations are progressing in the desired directions. This will open the door for correlation analysis to see if trends and historical data can be used to find additional information about operations.\nPhase III will open up the archives for users to select a specific week while developing and deploying other templates for users to access. The idea is to create a website that allows a user to specify a report from a series of templates and then select the dataset they wish to access. I would continue to use quarto to develop the website. That makes the most sense because it will already be present and I have experience with it, as evidenced by this site.\nPhase IV will focus on using my data engineering skills to build an ingestion pipeline to consume the archival data and make it available through the website to users. I think that I will use duckdb in the backend to create the dataframes from the data lake and then apply the data to pre-built templates to create more informative reports. I think that duckdb will allow me to leverage my SQL skills and be more productive with less lines of code. At this stage, I will also start building different dashboards using Streamlit and quarto as the backbones for them. I have reveiwed a book on Streamlit in the past, so I have a lot of familiarity with it and that will serve me well in developing the basics for different audiences. I want to create different dashboards for the executive suite, operations management, and floor supervisors. Each of the different groups will want to see different information that is relevant to them.\nOnce this has been solidified and the solution has demonstrated that it is working not only has I intend, but has matured with user feedback, then I will build the next and final expected phase. Phase V will give the user the opportunity to use a local LLM to build custom reports from a specified dataset. I think that this level of interacivity will allow users to really dive into the data and focus on using data to drive decision-making. I plan on deploying an LLM locally because I donâ€™t want to expose data to the outside world and I want to ensure that I can control costs by using local deployment. I have already built two laboratories, so I understand what I need and how to construct the backbone. Right now, Iâ€™m leaning toward using ollama because Iâ€™ve built two labs with it and I know how to configure it. I just need the RAM and the processor speed to run it properly.\nOnce the final phase has been completed and everything is stable, mature, and only needs the occasional code update, then my goal is to package this up and make it available to other centres. If I had my way, I would make it free to smaller centres for free and set up a payment scheme for larger centres so that could subsidize future development options.\nI already have my next post in the pipeline, so it should be released even sooner."
  },
  {
    "objectID": "posts/20250801/index.html",
    "href": "posts/20250801/index.html",
    "title": "You never know what is interesting",
    "section": "",
    "text": "About a month or so back, I had an opportunity to speak with several people from a company that makes software for public safety entities. I had connected with one of their folks through my BlueSky account. After a few preliminary conversations, they wanted to read my dissertation. Trust me folks, when someone asks to read it, you are truly excited. I sent them a copy of it and when it came time to meet with the group, I was excited to learn they really did read it!\nWhen we met, they asked me a few questions about it. Overall, they appeared to be impressed with my work. However, the surprise came when I mentioned a small piece of toy software I have written. They were immediately interested in this software. They were thinking of new uses for it, beyond my reason for creating it, and asking questions about what I developed and why I made the design choices that I dud. The conversation was very enlightening. I admit that I didnâ€™t expect that the program would be the subject of anyoneâ€™s interest.\nBecause I give many presentations around the country about statistics and data use, I use synthetic data in place of real data from a specfic centre for my presentations. It provides better compliance with CJIS and HIPAA regulations because Iâ€™m not potentially exposing anyoneâ€™s data. Granted, I donâ€™t ever use the call narrative, but some systems may identify telecommunicators by name, and I donâ€™t want to expose that either. In the past, I used Mostly.ai as my first provider. The building process was challenging, not because of any deficiencies on their part. They were kind enough to extend credits to me to dive into the product after I met with them and discussed what I was doing and why. When I created my first datasets using their product, I made mistakes and ended up creating a dataset that had a normal distribution for elapsed times between events. Granted, I will admit that was user error. So, I went through the NumPy library and started leveraging different random generators to find the right distribution for the data when compared to actual data from my center.I wrote an analyzer to look over the elapsed times for each interval and report back the distribution and its properties. Once I had those, I matched that to the proper random generator so I would have realistic elapsed times. After I created those, I used that to create my ancillary timestamps.\nWhen I started making it, I thought of it is as something that I could use to generate data for my speeches and a way to learn more about Python programming. I didnâ€™t conceive of any other uses or anyone else being interested. Maybe someone like Mostly might look at it and, possibly, give me suggestions to make it better or something like that, but I didnâ€™t expect anyone else to find uses for it or inspire me to see different uses for it. However, like the post about Asking the First Question, it was someone else who inspried me to take things in a direction I hadnâ€™t expected.\nI hope you find this post useful. Thanks for reading!"
  },
  {
    "objectID": "posts/20250415/index.html",
    "href": "posts/20250415/index.html",
    "title": "How to Start - Asking the First Question",
    "section": "",
    "text": "I recently wrote about a presentation that I gave at Randolph Macon College concerning using data analyses in 9-1-1 centres. During the Q&A section, someone asked me how I would recommend getting started. My answer, then and now, is pick a question and dive into that and new questions will start coming.\nI thought, perhaps, I should come up with an example of what I mean. In our centre, our medical director has requested that we do quality checks on every cardiac arrest call that we receive. So, here is the starting question: what can we learn about the cardiac arrest calls in the city? With that as the opening question, the first step is collecting the data. To start, I plan on collecting four datasets. I can create all of them using SQL. Since I work on SQL Server or T-SQL flavoured databases, the query, for our dispatch softwareâ€™s databases looks something like this:\n\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 = '2025-01-01 00:00:00.0000000';\n\n-- This query will retrieve all cardiac arrest calls from this year.\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n\n-- This query will retrieve  all cardiac arrest calls from the past 1, 3, & 5 years\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date BETWEEN DATEADD(YEAR, -1, @time1) AND @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n\nThese queries will generate the four datasets that I would want for the full analysis. I would save the output to csv files and name them cardiac_arrest_cy.csv, cardiac_arrest_1y.csv, cardiac_arrest_3y.csv, and cardiac_arrest_5y.csv.\nPersonally, I want to start with current data so I can get a feel for the data. To do some of my work, I would add some columns to the dataset. I can do it programmatically or through the SQL Query. I prefer to do it in the query like so:\n\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 '2025-01-01';\n\n-- This query will retrieve all cardiac arrest calls from this year\n\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    Address,\n    Latitude,\n    Longitude,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n    DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n    DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n    DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\n  AND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n\nThis gives us a columns of elapsed times to determine how long it took us to make the call dispatchable, how long it took to dispatch the call to the first unit, and how long we spent on the phone with the caller.\nNow we load the dataset, for this Iâ€™m using the R programming language. We can do it in Python as well, but Iâ€™ve been working with R a lot longer.\n\ndf_cacy &lt;- read.csv(\"cardiac_arrest_cy.csv\", header = TRUE, sep = \",\", stringsAsFactors = TRUE)\n\nNow that we have the dataset loaded, we can go through the dataset to clean it up. Most of these calls should have all of the components that weâ€™ve selected. If there are things missing, then we can go in and clean those up to remove missing data points. For this dataset, this is the code I would use to clean up any missing values:\n\n# Check the data\nstr(df_cacy)\nnrow(df_cacy)\nhead(df_cacy, n = 10)\ncolnames(df_cacy)\nspec(df_cacy)\n\n# Use the naniar package to check for missing values. This creates a graphical view of the missing data\ngg_miss_var(df_cacy)\n\n# Use this code to create a quick table of those missing value counts\napply(X = is.na(df_cacy), MARGIN = 2, FUN = sum)\n\n# This code replaces missing values with an entry\ndf_cacy$Call_ID &lt;- tidyr::replace_na(df_cacy$Call_ID, \"NOT RECORDED\")\ndf_cacy$Start_Time &lt;- tidyr::replace_na(df_cacy$Start_Time, \"1970-01-01 00:00:00\") # This is the Unix Time start value\ndf_cacy$Address &lt;- tidyr::replace_na(df_cacy$Address, \"NOT RECORDED\")\ndf_cacy$Latitude &lt;- tidyr::replace_na(df_cacy$Latitude, 388048)\ndf_cacy$Longitude &lt;- tidyr::replace_na(df_cacy$Longitude, 770469)\ndf_cacy$Ready_To_Dispatch &lt;- tidyr::replace_na(df_cacy$Ready_To_Dispatch, \"1970-01-01 00:00:00\")\ndf_cacy$First_Unit_Assigned &lt;- tidyr::replace_na(df_cacy$First_Unit_Assigned, \"1970-01-01 00:00:00\")\ndf_cacy$Stop_Time &lt;- tidyr::replace_na(df_cacy$Stop_Time, \"1970-01-01 00:00:00\")\ndf_cacy$Call_Entry_Time &lt;- tidyr::replace_na(df_cacy$Call_Entry_Time, -1) # This makes any elapsed time that is missing a value we can eliminate in the next cleaning step.\ndf_cacy$Call_Queue_Time &lt;- tidyr::replace_na(df_cacy$Call_Queue_Time, -1)\ndf_cacy$Call_Processing_Time &lt;- tidyr::replace_na(df_cacy$Call_Processing_Time, -1)\n\nThis should clean the data of any missing values. This is also the start of new questions. How much data is missing? Where is it missing at? Finally, why is the data missing? Most of the data is likely not missing at random. I expect that most of the missing data comes from calls that were not completed. When the other three benchmark datasets are compared, we can see how the percentage of calls missing data compares over 1, 3, and 5 years. If the comparisons are in line, then you move on, if they arenâ€™t, thereâ€™s question number 4, whatâ€™s different now?\nThe next thing that I check for would be negative values in the final three columns, Call_Entry_Time, Call_Queue_Time, and Call_Processing_Time. These varables represent the elapsed times between events in the centre that measure our telecommunicators actions. If we see any values in these variables that are negative, then we know, instantly, that the calls have been closed then reopened. The fifth questions becomes why are some calls being reopened? My recommendation is to take those calls, and the calls with missing values if they can be proven to be full calls, and create a new dataset. That list can be exported to an Excel spreadsheet and used for future investigation. This is the first potential project that has been generated by this work.\nDepending on the number of calls that exist in this dataset, I would remove the same calls from my master datasets and continue with the research without them. My benchmark is no more than 5 percent of the calls removed. If it appears that weâ€™re going to go over 5 percent, we have another question, why are there so many calls with these defects?\nIn a quick glance after running the queries, I found that none of my entries for this year had any missing data and that historical data had missing data in the Time_First_Unit_Assigned column. These would be calls that were received, but never ran for whatever reason. This is what I expected to see in real data.\nOnce the call list has been cleaned, the next step that I recommend is to look at individual variables. I prefer to start with the numeric variables and create a summary. I have a custom summary that I use for my analyses. I feel like it serves my needs and gives me some insights that I will confirm when I create visuals for others. That summary can be found here:\n\n# Custom summary function\ncustom_summary &lt;- function(column) {\n  c(\n    Minimum = round(min(column, na.rm = TRUE), 2),\n    Mean = round(mean(column, na.rm = TRUE), 2),\n    Median = round(median(column, na.rm = TRUE), 2),\n    Q1 = round(quantile(column, 0.25, na.rm = TRUE), 2),\n    Q3 = round(quantile(column, 0.75, na.rm = TRUE), 2),\n    P90th = round(quantile(column, 0.90. na.rm = TRUE), 2),\n    Maximum = round(max(column, na.rm = TRUE), 2),\n    Std_Dev = round(sd(column, na.rm = TRUE), 2),\n    Variance = round(var(column, na.rm = TRUE), 2),\n    Skewness = round(e1071::skewness(column, na.rm = TRUE), 2),\n    Kurtosis = round(e1071::kurtosis(column, na.rm = TRUE), 2)\n  )\n}\n\nThis custom summary gives me a very good picture of the data prior to creating any visualizations. The minimum and maximum values give me a good range for the data, while the mean and median paint a picture of the central tendencies of the data. The standard deviation and variance can give me some information about the overall spread of the data, the skewness and kurtosis values give me the shape of the distribution, and the Q1 and Q3 values suggest the boundaries for outlier identification. All of this together now gives me a good numeric â€˜visualizationâ€™ of the data and now I can focus on asking new questions of the numeric data.\nAdditionally, I will want to break the Response_Date column into some constituent components such as the day of the week, the day of the year, the hour, and the week number. These will allow me to create different pictures. To get those, I typically generate them from the SQL Query. That updated query would look like this:\n\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    CAST(DATEPART(WEEK, Response_Date) AS NVARCHAR(2)) AS [WeekNo],\n    UPPER(FORMAT(Response_Date, 'ddd')) AS [DOW],\n    CAST(DATEPART(DAY, Response_Date) AS NVARCHAR(2)) AS [Day],\n    CAST(DATEPART(Hour, Response_Date) AS NVARCHAR(2)) AS [Hour],\n    Address,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n  DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n  DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n  DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date &gt; @time1\nAND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n\nor you can do it programmatically in R like this:\n\n# Function to extract day of week (DDD), week number, year, and hour\n# This also uses the lubridate library\nextract_datetime_features &lt;- function(df_cacy, Response_Date) {\n  df %&gt;%\n    mutate(\n      day_of_week_ddd = wday(!!sym(Response_Date), label = TRUE),\n      week_number = isoweek(!!sym(Response_Date)),\n      year_number = year(!!sym(Response_Date)),\n      hour_of_day = hour(!!sym(Response_Date))\n    )\n}\n\nEither way, thsis allows us new data to generate new questions and answers. Is there a specific day of the week that has more events than others? Is there a certain time of the year that has more events? Are there specific hours out of the day that see more events? These questions could impact staffing, training, and could present opportunities to partner with the local hospitals to create better synergies for positive patient experiences and outcomes. So weâ€™re now at 8 questions from our starting point as well as a couple of different projects. And when you take a look at the summaries for each of the elapsed times and compare them to the 1, 3, and 5 year summaries, you can not only see trends but find new questions when doing the comparisons to those benchmarks.\nFrom here, for the more visually minded, we can create two or three visualizations for each of the three elapsed time columns. I recommend a histogram or density plot, a blox plot, and a QQ plot. The last one will show you, visually, what the skewness and kurtosis told you numerically. This code should create those for you. Just remember that youâ€™re doing it for each of the three variables.\n\n# Historgram and Density plt\nggplot(df_cacy, aes(x = Call_Entry_Time)) + \n  geom_histogram(aes(y = ..density..),\n                 colour = 1, fill = \"white\") +\n  geom_density(lwd = 1, colour = 4,\n               fill = 4, alpha = 0.25)\n\n# QQ Plot\nggqqplot(df_cacy, x = \"Call_Entry_Time\", color = \"#1c5789\", title=\"QQ Plot of TimeToQueue\", ylab = \"Call Entry Time\")\n\n## Box Plot\nbox1 &lt;- df_cacy %&gt;% ggplot(aes(x=DOW, y=Call_Entry_Time)) +\n  geom_boxplot() +\n  scale_fill_brewer(palette=\"Dark2\") +\n  ggtitle(\"Boxplot of Call Entry Times by Day\")\n\nbox1\n\nComparing the results of the current year to the benchmarks gives you visual verification of what youâ€™ve seen numerically. These are also great for the subsequent reports because your audience is now able to visualize what youâ€™re telling them. This may lead to additional questions about the internal working of the data. This could add to the question and project totals that you have generated.\nVisualizations are not simply for numeric data. We can create boxplots for the factor or categorical data. For example, this allows us to look at each day of the week and see how many cardiac arrest calls we have for each day of the week.\n\ndf_cacy$DOW &lt;- factor(df_cacy$DOW, levels = c(\"SUN\",\"MON\",\"TUE\",\"WED\",\"THU\",\"FRI\",\"SAT\"))\n\n# ggplot2\nbarDOW &lt;- df_cacy %&gt;% ggplot(aes(x=DOW, fill=DOW)) +\n  geom_bar() +\n  scale_fill_viridis(discrete = TRUE, option = \"H\") +\n  ggtitle(\"Count of calls per day of the week\") +\n  geom_text(\n    stat = 'count', \n    aes(label = after_stat(count)), \n    vjust = -0.5  # Adjusts the vertical position of the count\n  )\n\nbarDOW\n\nIf anything stands out in this, we can also check that against the benchmark datasets and determine if this normal or if itâ€™s standing out for a different reason. This can also be generated for each hour of the day with minor changes to the code. This leads us to another plot that can be visually appealing while also giving us some excellent information. We can use a ridgeline plot to give us a count of events per hour per doy. Are there specific times and days which see more events than others. Itâ€™s likely that we would see different patters if we examined traffic stops. I expect, if we ran this out, we would not see any particular combination stand out.\n\n# Aggregate data to get counts per hour per day\nhourly_calls &lt;- df_cacy %&gt;%\n  group_by(DOW, Hour) %&gt;%\n  dplyr::tally(name = \"CallCount\")\n\n# Create the Ridgeline Plot\nridge_plot &lt;- ggplot(hourly_calls, aes(x = Hour, y = DOW, height = CallCount, group = DOW, fill = DOW)) + # Add fill aesthetic\n  geom_density_ridges(stat = \"identity\", scale = 0.9, rel_min_height = 0.01) +\n    scale_x_continuous(breaks = 0:23) + # Set breaks for each hour\n  scale_y_discrete(limits = rev) + # Reverse order of days for better readability\n  scale_fill_viridis(discrete = TRUE, option = \"H\") + # Use viridis scale\n  labs(title = \"Call Count by Hour and Day of Week\",\n       x = \"Hour of Day\",\n       y = \"Day of Week\",\n       fill = \"Day of Week\") + # Add legend title\n  theme_ridges(font_size = 20, grid = TRUE)\n\nridge_plot\n\nAnother thing we can do is look at the geographic distribution of cardiac arrests in the city. Where do we have more of them? that might help us position our resources better and assist us with patient care by ensuring that people at those locations are properly trained in CPR to help get hands on chest faster which increases the likelihood of survival. This code will create and display a map of the City with the locations marked. Hovering over any individual data point will also give us call information so we can see other details.\n\n# Convert integer coordinates to decimal degrees\n# For latitude: Divide by 1,000,000 and keep positive (Northern hemisphere)\n# For longitude: Divide by 1,000,000 and make negative (Western hemisphere for Alexandria)\ncalls_with_coords &lt;- df_cacy %&gt;%\n  mutate(\n    lat = Latitude / 1000000,\n    long= -1 * (Longitude / 1000000)  # Negative for western hemisphere\n  )\n\n# Create the map\nalexandria_map &lt;- leaflet(calls_with_coords) %&gt;%\n  # Add base map tiles\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  # Set the view to center on Alexandria, VA\n  setView(lng = -77.0469, lat = 38.8048, zoom = 13) %&gt;%\n  # Add markers for each call\n  addCircleMarkers(\n    ~long, ~lat,\n    color = '#1c5789',\n    radius = 6,\n    stroke = FALSE,\n    fillOpacity = 0.7,\n    popup = ~paste(\"&lt;b&gt;Call ID:&lt;/b&gt;\", Master_Incident_Number, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Date:&lt;/b&gt;\", Response_Date, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Address:&lt;/b&gt;\", Address, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To Queue:&lt;/b&gt;\", Call_Entry_Time, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To DispatchL&lt;/b&gt;\", Call_Queue_Time, \"&lt;b&gt;\",\n                   \"&lt;b&gt;Seconds On Call:&lt;/b&gt;\", Call_Process_Time, \"&lt;br&gt;\")\n  ) %&gt;%\n  # Add a title\n  addControl(\n    html = \"&lt;h3&gt;Cardiac Arrest Calls&lt;br&gt;Alexandria, VA&lt;/h3&gt;\",\n    position = \"topright\"\n  )\n\n# Display the map\nalexandria_map\n\n# Save the map for inclusion in reports\nsaveWidget(alexandria_map, \"alexandria_calls_map.html\", selfcontained = TRUE)\n\n# Optional: If you want to analyze call density by neighborhood or district\n# This creates a clustered view that can help identify hotspots\ncluster_map &lt;- leaflet(calls_with_coords) %&gt;%\n  # Add base map tiles\n  addProviderTiles(providers$CartoDB.Positron) %&gt;%\n  # Set the view to center on Alexandria, VA\n  setView(lng = -77.0469, lat = 38.8048, zoom = 13) %&gt;%\n  # Add clustered markers\n  addMarkers(\n    ~longitude, ~latitude,\n    popup = ~paste(\"&lt;b&gt;Call ID:&lt;/b&gt;\", Master_Incident_Number, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Date:&lt;/b&gt;\", Response_Date, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Address:&lt;/b&gt;\", Address, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To Queue:&lt;/b&gt;\", Call_Entry_Time, \"&lt;br&gt;\",\n                   \"&lt;b&gt;Seconds To DispatchL&lt;/b&gt;\", Call_Queue_Time, \"&lt;b&gt;\",\n                   \"&lt;b&gt;Seconds On Call:&lt;/b&gt;\", Call_Process_Time, \"&lt;br&gt;\"),\n    clusterOptions = markerClusterOptions(),\n    label = ~as.character(Master_Incident_Number)\n  ) %&gt;%\n  # Add a title\n  addControl(\n    html = \"&lt;h3&gt;Clustered Call Locations&lt;br&gt;Alexandria, VA&lt;/h3&gt;\",\n    position = \"topright\"\n  )\n\n# Uncomment to save the cluster map\n# saveWidget(cluster_map, \"alexandria_calls_cluster_map.html\", selfcontained = TRUE)\n\nWe have set this up to show us where all of the calls are and when we hover over an individual call, it will give us information about the call including the master incident number, the address, the date and time of when it was generated, and the amount of time that passed before it went to queue, was dispatcahed, and how long we were on the call. Seeing the geographic clusters will likely identify nursing homes or other care facilities, so if we see clusters in other places, we may want to look into why a cluster of cardiac arrests are there. These clusters can also be compared against the benchmarks to see if they look the same. If they donâ€™t, we can start asking why we have a new cluster. That increases the number of questions that we can ask and the number of projects we have in front of us.\nA final example of a new research project would be to create a binary variable in hand with deeper research to determine if calls were originally coded as cardiac arrests or if they were coded and dispatched as something else and changed to cardiac arrests later in the process. That examination could show how accurate we were in triaging and handling these calls. I will leave that code for another day and another post.\nFrom here, you can go anywhere you want to and find answers to the questions that youâ€™ve found. I hope that this showed exactly how you can start with one question, what can we learn about cardiac arrests, and turn it into many questions and projects that show how much is left to know."
  },
  {
    "objectID": "posts/20250204/index.html",
    "href": "posts/20250204/index.html",
    "title": "Directions for Future Posts",
    "section": "",
    "text": "Iâ€™ve been considering the"
  },
  {
    "objectID": "posts/20250106/index.html",
    "href": "posts/20250106/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "With the snow storm that moved from the midwest over the Appalachians and through Virginia, I received texts and a phone call from a friend and former colleague. She asked if I knew of a way to view all of the current calls for service throughout the state. No, sheâ€™s not crazy, she was investigating a request made of her. However, the commiunication started me thinking about an â€œend-to-endâ€ data science solution that could make that a reality.\nThwew are severak steps that need to be taken to create such an application. Granted, the end result would be focused"
  },
  {
    "objectID": "posts/20241223/index.html",
    "href": "posts/20241223/index.html",
    "title": "Thanks and Thoughts",
    "section": "",
    "text": "Well, the site is now live, thanks to Ionosâ€™ technical support. I have to compliment the person who followed up with me today. He helped me get everything taken care of and get the site live and available for you to see. Itâ€™s nice to go from working to build it to seeing it out there where everyone else can see it and we can go on this journey together.\nI also want to thank Beartiz Milz for writing an awesome tutorial that helped me turn my limited knowledge of Quarto and my comfort with R into a website and blog to promote my work in 911 data science. Iâ€™m also using her tutorial to build a blog as a vehicle for new reports in my PSAP.\nFor the last 14 years, Iâ€™ve written reports for two different PSAPs and felt like my reports have not illuminated everything that the data has to offer. The weekly and monthly reports have been things Iâ€™ve been asked to develop. However, there is always so much more in the data which could be important. Can we use knowledge about what days and hours are busiest change how we schedule telecommunicators? Can we see training oportunities by focusing on how long it takes us to process specific call types? Are there hidden correlations in the data that weâ€™ve been missing because we donâ€™t see them? Iâ€™ve thought of these questions forever. Now that Iâ€™ve finished my doctorate, I want to spend the time investigating these and other questions.\nI think that I can build templates that I can share with other interested PSAPs and we can start building better reporting systems which can, by being served as dashboards and websites, make the reports more responsive while automating the delivery. I want to mke things easier for everyone.\nI updated my Medium site with an annoucement that this blog exists and has become active. Iâ€™ve also updated LinkedIn with a link to that article and invited my friends and contacts there to add comments and ask questions that can turn into blog posts. I will post this article there too, once Iâ€™ve published it.\nSo, this is where this post ends. I will post something new in a couple of days. Iâ€™m going to challenge myself to add some code in the next one to see what happens. I have a couple of ideas in mind."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Dr.Â D Public Safety Data Science",
    "section": "",
    "text": "Deception in Graphics and Statistics\n\n\n\ndata science\n\nanalyses\n\ndata integrity\n\ndeceptive statistics\n\n\n\n\n\n\n\n\n\nNov 22, 2025\n\n7 min\n\n\n\n\n\n\nStochasticity in Time-Series, Dissertation part 1\n\n\n\ndata science\n\nanalyses\n\ndata issues\n\n\n\n\n\n\n\n\n\nOct 27, 2025\n\n7 min\n\n\n\n\n\n\nChronomancer Prestige Class 5e\n\n\n\nDungeons & Dragons\n\nWizard\n\nSubclasses\n\n\n\n\n\n\n\n\n\nOct 26, 2025\n\n2 min\n\n\n\n\n\n\nCleaning in the Query\n\n\n\ndata science\n\nanalyses\n\ndata issues\n\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n8 min\n\n\n\n\n\n\nGetting Certified\n\n\n\ndata science\n\ncertification\n\nstudying\n\n\n\n\n\n\n\n\n\nAug 30, 2025\n\n5 min\n\n\n\n\n\n\nTaking care of the community\n\n\n\ndata\n\ncommunity\n\ndispatch\n\n\n\n\n\n\n\n\n\nAug 25, 2025\n\n1 min\n\n\n\n\n\n\nBuilding a data-centric future\n\n\n\nresearch. data-driven\n\ndata\n\n\n\n\n\n\n\n\n\nAug 19, 2025\n\n5 min\n\n\n\n\n\n\nRemembering My Target Audience\n\n\n\ndevelopment\n\nsoftware\n\n\n\n\n\n\n\n\n\nAug 17, 2025\n\n13 min\n\n\n\n\n\n\nBuilding a new reporting system\n\n\n\ndevelopment\n\nsoftware\n\n\n\n\n\n\n\n\n\nAug 10, 2025\n\n4 min\n\n\n\n\n\n\nYou never know what is interesting\n\n\n\nupdates\n\nnews\n\n\n\n\n\n\n\n\n\nAug 4, 2025\n\n3 min\n\n\n\n\n\n\nNENA 25 National Convention - Long Beach\n\n\n\nupdates\n\nnews\n\n\n\n\n\n\n\n\n\nJul 21, 2025\n\n2 min\n\n\n\n\n\n\nHow to Start - Asking the First Question\n\n\n\nexamples\n\nanalyses\n\n\n\n\n\n\n\n\n\nApr 12, 2025\n\n17 min\n\n\n\n\n\n\nUpdated Directions\n\n\n\nnews\n\nthoughts\n\nupdates\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n4 min\n\n\n\n\n\n\nDirections for Future Posts\n\n\n\nthoughts\n\nideas\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n1 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\neditorial\n\ndata science\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n4 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\nfull-stack\n\ndata science\n\n\n\n\n\n\n\n\n\nJan 6, 2025\n\n1 min\n\n\n\n\n\n\nSo where does the data take me?\n\n\n\nnews\n\nanalytics\n\n911\n\n\n\n\n\n\n\n\n\nJan 5, 2025\n\n1 min\n\n\n\n\n\n\nThanks and Thoughts\n\n\n\nnews\n\nthoughts\n\nreports\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n3 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am Dr.Â Tony Dunsworth. I have been working with data in public safety answering points (PSAPs) for nearly 15 years. Over that time, I work to find new ways to use the data generated by PSAPs to help a center be better staffed, more responsive to the community, and to help the community discover issues that can be addressed to reduce calls to the center. During my tenure in public safety, Iâ€™ve worked as a CAD manager, DBA, Data Analyst, and a Data Scientist.\nI completed my PhD work at National University in Data Science. Most of my research concerned both Public Safety Emergency Communications Centers 911/999/112 systems and time series forecasting models. My dissertation evaluated the efficacy of automated forecasting libraries like Prophet or TimeGPT in building accurate forecasts for noisy multi-stochastic datasets like Emergency Communication Center call volumes."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\nNational University | La Jolla, CA PhD in Data Science | July 2021 - October 2024\nWestern Governors University | Salt Lake City, UT M.S. in Data Analytics | September 2017 - May 2021\nUniversity of Phoenix | Kansas City, MO Masters in Information Systems | February 2005 - May 2007\nWestern Governors University | Salt Lake City, UT B.S in Software Development | September 2012 - June 2017\nUniverity of Kansas | Lawrence, KS B.A. in Linguistics | August 1987 - May 1991"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\nCity of Alexandria | Database Administrator II | December 2014 - present\nJohnson County Sheriffâ€™s Office | CAD Administrator | Sept 2010 - November 2014"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "I am a data scientist focusing on the use of data to improve emergency operations in our communities. I also research zero-shot time series forecasting libraries. I want to develop automated analytical libraries to be used in 911/999/112 centers to give staff access to better intelligence through the application of advanced techniques."
  },
  {
    "objectID": "posts/20250105/index.html",
    "href": "posts/20250105/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "So, over the dead week between Christmas and New Yearâ€™s Day, I started working on a new reporting template. The goal is to create a new archive of weekly and monthly reports for my 911 centre in a Quarto blog format. Currently, Iâ€™m working on a sample report for management to see what the audience will like. I also want to use the sample to illustrate what needs to happen with the reports.\nAn example of this is in our datasets, we trap the time stamps at various points in the life of the service call. Some of these include the start of the call, when it was available to be dispatched, when the first units were assigned, and when the call was disconnected. In the data cleaning process, we have identified some of the deltas between time stamps come out negative. Obviously, you cannot dispatch a call before it enters the queue. This means that when we find them, we have to address them in some fashion."
  },
  {
    "objectID": "posts/20250128/index.html",
    "href": "posts/20250128/index.html",
    "title": "So where does the data take me?",
    "section": "",
    "text": "I just finished a week at NENAâ€™s Standard and Best Practices. I had a wonderful time with my presentations on developing staffing models, data management, and using data better in call processing. I met a lot of new friends and I see a lot of opportunities ahead of me to make a real difference at the intersection of public safety and data science.\n\n\nI participated in seven presentations. I will upload slide decks and notes later for people who are interested in learning what I was talking about in detail.\n\n\nThree of these were in a block discussing the need for a new staffing model recommendation in 911 centres. Currently, NENAâ€™s Staffing Recommendations are over 21 years old and rely heavily on the Erlang-C model for staffing recommendations. There has been a tonne of work in queuing theory since the development of this formula. And, as referenced in the linked paper, there are legitimate questions about how appropriate the Erlang model family is for regular call centres, let alone 911 centres.\nOne of the central problems is that Erlang-C models assume there is only one queue that delivers all of the customers to the servers. In most modern centres, there are at least 4 ingression points to the server set. We have dedicated 9-1-1 trunk lines, typically most primary PSAPs also answer some non-emergency lines, and most centres also take 9-1-1 SMS messages. Finally, all centres have to be able to receive and process TTD-TTY calls. Many larger centers may also process traffic cameras, ASAP2PSAP generated alarm calls, or other monitoring services like ShotSpotter. Dispatchers also have radio traffic from the units that they monitor which require their attention. So, there are numerous queues to access the servers, the servers are all multi-skilled servers, and the different queues have different priorities for servers depending on skill sets and responsibilities. Each of these adds levels of complexity that could confound an Erlang-C model.\nThere are several different methods that could come into play, including Markovian models, Game Theory, and Priority Queuing Models. Each of these can be explored and addressed.\nI remain convinced that we have enough brain power in NENA that our working group can tackle this issue and come up with a queuing model that will work for the industry.\n\n\n\nTwo of the other presentations centered around the Emergency Incident Data Object. This is how, we propose, to move information between different systems and entities in public safety. Itâ€™s in a JSON object that leverages NIEM to define the objects passed between entities. My presentations dicussed the JSON object itself and the conveyance mechanisms by which the JSON objects are moved between entities.\nI believe in the use of the EIDO as we continue to roll out NG9-1-1 services across the country. We have done a lot of good work throughout the years to create something that can be useful in ensuring data gets to where it needs to go and can be processed efficiently.\n\n\n\nThe other presentations in which I participated were the Call Processing working group where I am the â€˜data SMEâ€™ and the Future Think committeeâ€™s Data Management and Presentation work. That was a fun panel discussion with a lot of great audience participation. There is a general agreement that we need to use more of our data and use it more effectively.\nI am heartened, as a data scientist working in public safety, to hear my colleagues in public safety acknowledging the need to use data analytics, regression modeling, predictive analytics, and forecasting to improve things for telecommunicators.\nThere is also a movement to start considering a shared data repository for research and analytical purposes. I beieve that will be an excellent idea. If we can get several PSAPs together and build out a research database, we could put it in front of other researchers and see where it leades us.\n\n\nI changed the link for NIEM to go to the About page to give readers a better explanaiton of what NIEM is and why itâ€™s important. I also linked to a page describing JSON in better terms. Finally, my editor found a spelling error that was also corrected."
  },
  {
    "objectID": "posts/20250128/index.html#nenas-sbp",
    "href": "posts/20250128/index.html#nenas-sbp",
    "title": "So where does the data take me?",
    "section": "",
    "text": "I participated in seven presentations. I will upload slide decks and notes later for people who are interested in learning what I was talking about in detail.\n\n\nThree of these were in a block discussing the need for a new staffing model recommendation in 911 centres. Currently, NENAâ€™s Staffing Recommendations are over 21 years old and rely heavily on the Erlang-C model for staffing recommendations. There has been a tonne of work in queuing theory since the development of this formula. And, as referenced in the linked paper, there are legitimate questions about how appropriate the Erlang model family is for regular call centres, let alone 911 centres.\nOne of the central problems is that Erlang-C models assume there is only one queue that delivers all of the customers to the servers. In most modern centres, there are at least 4 ingression points to the server set. We have dedicated 9-1-1 trunk lines, typically most primary PSAPs also answer some non-emergency lines, and most centres also take 9-1-1 SMS messages. Finally, all centres have to be able to receive and process TTD-TTY calls. Many larger centers may also process traffic cameras, ASAP2PSAP generated alarm calls, or other monitoring services like ShotSpotter. Dispatchers also have radio traffic from the units that they monitor which require their attention. So, there are numerous queues to access the servers, the servers are all multi-skilled servers, and the different queues have different priorities for servers depending on skill sets and responsibilities. Each of these adds levels of complexity that could confound an Erlang-C model.\nThere are several different methods that could come into play, including Markovian models, Game Theory, and Priority Queuing Models. Each of these can be explored and addressed.\nI remain convinced that we have enough brain power in NENA that our working group can tackle this issue and come up with a queuing model that will work for the industry.\n\n\n\nTwo of the other presentations centered around the Emergency Incident Data Object. This is how, we propose, to move information between different systems and entities in public safety. Itâ€™s in a JSON object that leverages NIEM to define the objects passed between entities. My presentations dicussed the JSON object itself and the conveyance mechanisms by which the JSON objects are moved between entities.\nI believe in the use of the EIDO as we continue to roll out NG9-1-1 services across the country. We have done a lot of good work throughout the years to create something that can be useful in ensuring data gets to where it needs to go and can be processed efficiently.\n\n\n\nThe other presentations in which I participated were the Call Processing working group where I am the â€˜data SMEâ€™ and the Future Think committeeâ€™s Data Management and Presentation work. That was a fun panel discussion with a lot of great audience participation. There is a general agreement that we need to use more of our data and use it more effectively.\nI am heartened, as a data scientist working in public safety, to hear my colleagues in public safety acknowledging the need to use data analytics, regression modeling, predictive analytics, and forecasting to improve things for telecommunicators.\nThere is also a movement to start considering a shared data repository for research and analytical purposes. I beieve that will be an excellent idea. If we can get several PSAPs together and build out a research database, we could put it in front of other researchers and see where it leades us.\n\n\nI changed the link for NIEM to go to the About page to give readers a better explanaiton of what NIEM is and why itâ€™s important. I also linked to a page describing JSON in better terms. Finally, my editor found a spelling error that was also corrected."
  },
  {
    "objectID": "posts/20250406/index.html",
    "href": "posts/20250406/index.html",
    "title": "Updated Directions",
    "section": "",
    "text": "It has certainly been way too long since Iâ€™ve written a blog post. I know that I promised myself and my readers, if I have any left, that I would do better with this, but here we are. Life has a funny way of getting in the way when you least expect it. However, those same recent events have given me considerable fodder for new posts that could be in the pipeline.\nIâ€™ve been working on a few projects lately. Many of those projects are going to be the bases of many future posts. The first of those is a piece of software that Iâ€™m working on`to generate more realistic synthetic data for use in presentations and data science for public safety. The problem with the current iteration of the software is that I donâ€™t like how the distributions are appearing. Iâ€™m working on different distributions to create one that I believe will be more representative of actual centre data.\nthe other large project on the deck is called Itâ€™s About Time! Time is essential in every part of any 9-1-1 service call. It starts with a time stamp at every step in the call process. The passage of time can determine a callâ€™s outcome. We can go further and ask how one event can influence another. Can the time spent at a specific point in the process tell you how likely the event will be successful? Can we see how well we meet our SLAs to partner organizations by viewing how well we handle the setup?\nIn each CAD on the market, calls that have been closed can be re-opened or cloned into a new call. The preferred method is to clone the call to preserve the timestamps in the original call. When a call is re-opened, some of the timestamps associated with it will change. This can create very large values and can create negative values when subsequent events seem to occur prior to the prior values. Both of these situations can throw off a centreâ€™s statistics and alter the perception of how it performs. Now that those skewed values are in place after a re-opened call, subsequent analysis must employ amelioration techniques to address those outlying values.\nI am also involved in a group that is looking to build a new staffing recommendations model for 9-1-1 centres. The organisation with which I volunteer, NENA, is looking to replace a 20+ year old study of staffing models with something that is balanced, appropriate for PSAPs of all sizes and call volumes. We also want to look at other factors that can identify good dispatchers and then find better ways to keep them.\nI have been working, for some time, on creating a new series of reports for the centre I work in. However, I think I want to extend that another level. I think that I would like to create software that would allow a centre to select a csv file, pass it like a parameter to the software and get an analysis in return. I think that I would want to make a stand-alone version and a cloud version. The cloud version would be less expensive in return for use of the data, after anonymization, for additional research projects. I will start building my lists of must-have, want-to-haves, and would-be-nice-to-haves. From there, I will design everything from the ground up and then we will see where it goes from there.\nSo, dear readers, if you have any comments, questions, thoughts, or cards to the players; feel free to let me know. I would like to hear from you and see where we can go next."
  },
  {
    "objectID": "posts/20250704/index.html",
    "href": "posts/20250704/index.html",
    "title": "NENA 25 National Convention - Long Beach",
    "section": "",
    "text": "Once again, I havenâ€™t written in a long time. I know Bad Dr.Â D! I have not been as diligent as a blogger as I would like to be. So now that my mea culpa is out of the way, letâ€™s proceed to the next step in this blog, actually discussing information.\nLast week, I was out in Long Beach, CA at the NENA 2025 Annual Convention at the Long Beach Convention Center. It was a bit of an â€œold homeâ€ week for me. I used to live near the convention center over 20 years ago. Obviously, some things were completely differnt while some things have not changed. In a way, it was nice to be back in the area. I was able to take the old Green Line and the Blue Line to get down to Long Beach from the airport.\nHowever, once the convention started, I found myself a lot busier than I expected I would be. I had two presentations to give. I knew that going in. What surprised me was the number of people who walked up to strike up a conversation. I am sitll not used to that one. My wife tells me that she doesnâ€™t know why I am so surprised, Iâ€™ve been working on so many different things for the City and the community that I shouldnâ€™t be surprised that people want to get a piece of my brain. I also had some really interesting and substantive meetings with several people. That helped me get some good directions for future research.\nIronically, one of the best meetings during the week had nothing to do with the convention. I was contacted by a colleague from private industry regarding a topic that has become a passion for me. Iâ€™ve started working on trying to build new staffing models for 9-1-1 centers. To that end, I lead a working group that is working on developing a new staffing model workbook. So, I have been talking to"
  },
  {
    "objectID": "posts/20250807/index.html",
    "href": "posts/20250807/index.html",
    "title": "Building a new reporting system",
    "section": "",
    "text": "I have a lot of projects in the works, probably too many to be honest. However, at the day job, the one that has been bouncing around the longest and is now the centre of my attention is building an updated reporting structure. Currently, most of the regular reports that I generate are call lists that fit certain criteria. This has satisfied management at multiple levels for a while now, however, I havenâ€™t been satisfied with it. I have wanted to change things up and bring more data into the reports. Iâ€™ve also wanted to expand the comparative statistics to determine where, if at all, correlations exist in operations. It is also, in my opinion, good to discern any trends that could be present in the data. Something call lists may miss.\nThe whole project will be deployed in five phases. The first phase is building the infrastructure. I know that may sound boring, but my experiences in engineering and analytics is that most of the job is boring to most people. I donâ€™t mind it at all, so Iâ€™m happy to take my time. The first decision is that of programming language. While I, typically, prefer R over Python, in this instance, Iâ€™m reversing that decision because I believe that I can accomplish the other phases more efficiently code-wise in Python. I would like to try this in R as well, but that is a project for another day. When this is done perhaps.\nOne of the reasons that I chose Python is my preference to use uv to manage the environment. Written in Rust, uv allows me to use one tool to manage the environment, install the preferred libraries, including the dependencies, and ensure the proper version of Python has been installed. It can also ensure that the project could be deployed on any platform by using .toml files to define all of the libraries and show the version of Python that is needed for the project. Iâ€™ve used other options, including Poetry, Anaconda, and pipenv, etc. However, Iâ€™ve found uv quicker and easier to use.\nAfter that, I selected my libraries. I chose to replace pandas with polars. Like uv, polars is written in Rust. No, Iâ€™m not planning on using Rust yet. I do know that Rust can build some very good tools for Python though. Obviously, Iâ€™m going to use NumPy for a lot of my mathematical and statistical work. I will also add in SciPy and Statsmodels. They both can contribute a lot to analyses and modeling. For grpahics I prefer to use either Plotly or Plotnine. I lean more toward Plotnine because it implements the grammar of graphics, the book behind the idea The Grammar of Graphics by Leland Wilkinson is the foundation of ggplot2 in R also. I like it because it is clear to implement and everything makes sense once you get used to it. The final two libraries that I want to ensure I implement in Phase I are Pydantic for type checking. That will ensure that my code doesnâ€™t create any type conversion problems, and quarto for publishing. It is wonderful for working wtih either pandoc or LaTeX to create high quality documents. It even now supports typst which uses Markdown for typesetting.\nThese choices will allow me to start building the infrastructure and start with the first step of creating reports that will better reflect the operations in the center. I also plan on starting to build in comparative analytics by comparing the week Iâ€™m analyzing to the previous week, 4 weeks prior, and the year prior. Eventually, I want to build comparisons that span at least 6 to 8 weeks. This will allow us to investigate and illuminate trends over time. I also want to perform correlative analysis over time to find other trends and patterns in the data. I think that could unlock information which would enable us to serve our community better. The other part of a weekly analysis, beyond the summary statistics and these other analyses, should be looking over KPIs that are applicable to the centre and see how well we are meeting those KPIs.\nWhen I get this phase completed and weâ€™ve evaluated the output and adjusted the reports in response to the feedback received, then I will head to Phase II. Phase II will improve the data pipeline. I would like to move from CSV files to direct database access and then refactor Phase I to use the new data pipeline. After that, I will start determining which KPIs I need to build dashboards for management. I will discuss this in another post.\nFor now, on to building the infrastructure and getting dug into Phase I. Happy Reading and please feel free to comment!"
  },
  {
    "objectID": "posts/20250824/index.html",
    "href": "posts/20250824/index.html",
    "title": "Building a data-centric future",
    "section": "",
    "text": "As most of you know, I have a few outside activities where I can use my skills to better things around me. For example, I contribute to first-look book reviews for Manning Press. I am a subject matter expert for CompTIA on a couple of different exams. I also volunteer my time and energy for NENA, the National Emergency Number Association. I work on standards and best practice development there. Recently, at their annual convention in Long Beach, CA., I had an idea to suggest something new. The new NENA board president, Lee Ann Magoski, challenged all of us in her inauguration speech to use more of the data we generate and use it more effectively. I started thinking about submitting a form to create a working group that could assist in completing data collection and analysis for other working groups that need it to further their work on standards and best practices. I ran my idea past some colleagues and I was pleasantly surprised to hear a lot of support. That support extended to colleagues offering their advice and support to help me with the paperwork and ensure that the community can be built and sustained.\nWhat started out as an idea and a form to create a working group got its first potential boost when I asked to speak to a NENA executive and was encouraged to think even bigger than the two working groups. Since it hasnâ€™t been approved yet, Iâ€™m being a bit circumspect. However, I want to discuss what I want to see for the future. I was redirected from creating a working group that could come together to support other groups, to developing a proposal to create a committee that can create working groups to support current and future data efforts. However, it can also do much more.\nNENA is an organization dedicated to educaiton, training, and the development of the industry. I was encouraged by an old mentor to contribute to them because he believed that I knew too much to be sitting in a cube all day watching everything pass by. During both my work in NENA and my development as a data scientist, Iâ€™ve wanted to find ways to advance the use of analytics and data science in the 911 industry. So, I have been creating that proposal to leverage both my passion for data analytics and data science and NENAâ€™s reputation for education and training to create new opportunities for the industry. To do that, Iâ€™ve started expanding the idea of the committee to do more than supply analytics to other working groups. I think that the committee could work with NENA to create more a more detailed census of the industry. I think we could become a clearing hours for data that could be used for many other research projects.\nI also believe that the committee could further education in our industry by helping PSAPs set up their own analytics systems, helping define best practices and giving instructions on how to build systems on a budget, using open source freeware components. Yes, I know some of our vendor partners might have a problem with parts of this, but I donâ€™t see this as something that would be truly negative for them. They could use this as a way to show their capabilities to the industry. Some centers will still want to purchase their produts because they donâ€™t want to set everything up for themselves. For those that do, this gives the vendors exposure to make sales. I also believe that the committee can contribute to future editions of the ENP and CMCP exams. I think that both certifications could benefit from basic statistical knowledge. I think that NENA can also use the committee to sponsor research projects that can benefit the community. One such research project that comes to mind comes from a report that the NFPA comissioned for the NFPA 1225 standard. That report can be found here. In this report, the authors suggested that their work could serve as a tool to guide future research. After reading their survey questions and their report, I think that this type of committee could spawn a working group interested in taking up that research using data that NENA can collect from PSAPs around the country. I also beleive that many more PSAPs would participate if they knew that NENA was behind the research efforts because we are the community and weâ€™re doing this to support our industry.\nDuring a different conversation where I have been working to get data for the two working groups to which Iâ€™ve alluded above, I was speaking with a state 9-1-1 administrator and she was intrigued about the opportunities that could come from NENA sponsoring research. Her thoughts were about NENA partnering with some university to ensure that research could be overseen by the universityâ€™s Institutional Research Board. That intrigued me because I had not thought of that at the outset. After that I started looking at what would be needed to create an IRB and found that NENA could do that on its own. This committee could help create that IRB and use that to allow for NENA sponsored human subject research. I think all of this benefits NENA and the industry that it represents.\nIâ€™m hoping that the proposal will be adopted after itâ€™s submitted. I know that it has several advocates already and I think that will benefit all of us for years to come."
  },
  {
    "objectID": "posts/20250826/index.html",
    "href": "posts/20250826/index.html",
    "title": "Getting Certified",
    "section": "",
    "text": "As my readers know, from following this blog, I work with CompTIA on developing certification exams for Database Administrators and Data Analysts. One of the criteria for sunject matter experts is that we hold a valid CompTIA certification outselves. Thankfully, they were gracious enough to suspend that for me while I was finishing my Ph.D.Â Now that Iâ€™ve finished it, I know that I have to start working on obtaining a certification. At one point, I held four CompTIA certfications, but I sadly allowed them to lapse because I was working on a Masters in Data Analytics. The ones that I held were the A+, the Network+, the Security+ and the Project+ certs. I suppose that, with a little review time, I could retake the exams and obtain all four of them. However, I wonâ€™t make use of them in my day job any longer, so it feels like it defeats the purpose.\nWhile we work on the exams, CompTIA reminds us to ensure that we focus on things that the target audience would acutally need to know or do in their daily work. I think that is an excellent idea. So, Iâ€™m going to follow the same advice for the selection of the certification I am choosing to pursue. In this case, Iâ€™m starting to study for the DataX exam. I applied to work on this exam, but I was told that my qualifications were insufficient. At first, I was surprised because I thought that my resume was impressive enough to work on this exam. In retrospect, I can see where I fell short. While I had a lot of experience in data analytics and statistics, I wasnâ€™t working as a data scientist on a daily basis, so I was just a bit short. I might reapply after I get the certification since I think the combination of passing the exam and having the Ph.D.Â might qualify me.\nUntil then, like anyone else, I have to study to prepare for the exam. So far, to do that, I picked up a book of questions from Amazon and have it on my Fire tablet. I know that most of the questions are not in the CompTIA format, but itâ€™s a good way to review different topics that may be covered on the exam. Iâ€™ve also purchased a Udemy course for the exam. Iâ€™ve used some of them in the past to help me study and they were effective when I made good use of them. So thatâ€™s an emerging discipline issue that I will address.\nThe first exam objective, according to the CompTIA document Iâ€™ve linked above is mathematics and statistics. It makes sense to me since math is the foundation of analytics and data science. In this domain, most of the topics make sense and I can see how they apply. The question is going to be how in depth do they cover p-values? Everything else in there seems to make sense to me. I think that I will need to review AIC/BIC, type I and II errors, and a few other statistical concepts. The second objective in the domain is probability and synthetic modeling. A lot of this is stuff that I use daily, types of distributions, missingness, skewness and kurtosis. Those two are big with my work on 9-1-1 data. I know that I will want to cover homoskedacity and heteroskedacity in depth. I also need to review Probability Density Function and Probability Mass Function. The third objective in this domain is linear algebra and calculus. I know I need to review that. I have always been a much better statistician than mathematician. I will find additional resources to augment my education in this area. The final objective in this domain is temporal. I know that I will feel comfortable there. I wrote a dissertation on the subject and have built many models I thought that it was interesting to see that survival analysis is included here. I also saw that the objectives list a difference between parametric and non-parametric survival analyses. I want to ensure that I look into this a bit more.\nThe second domain covers modeling, analyses, and outcomes. The first objective that exists in this domain is Exploratory Data Analysis (EDA). I think that every data scientist has conducted EDA at least a dozen times in their practice. Itâ€™s the first thing that we do with new datasets. All of the topics that are covered under this objective should be pretty straightforward for me and my studies. There are the different types of analyses and the different plots that can be used for visualizing the data. The second objective centers on common data issues such as sparse data, outliers, lagged observations, multicollinearity issues, and other issues. The third objective covers data enrichment such as geocoding, feature engineering, and transforming data. I have done quite a bit of feature engineering when I have built forecasts. I also like that synthetic data is covered here. Iâ€™ve done a lot of work recently with synthetic data, so I will feel comfortable there. I think that I want to review a lot of data transformation techniques prior to the exam. Mainly that will be needed because that is something that I donâ€™t do as much of that in my current work. That doesnâ€™t mean that I donâ€™t need to do it, just that Iâ€™ve not applied these much.\nIâ€™m going to continue to cover this in more depth while I work on my studies, but this is a good start to get me moving. I have other blog posts that I want to get out to all of you. So, on to the next step."
  },
  {
    "objectID": "posts/20250917/index.html",
    "href": "posts/20250917/index.html",
    "title": "Stochasticity in Time-Series, Dissertation part 1",
    "section": "",
    "text": "Itâ€™s been nearly a year since I defended my dissertation. In that time, Iâ€™ve found that people are intrested in knowing what itâ€™s about until I try to explain it, then they feel that they donâ€™t understand it and the discussions get lost. So, while I was playing FreeCell solitaire this morning, I thought of an experiment that could help me work with forecasting stochastic processes, including adding conformal predictions to the mixture. Part of it is because I want to keep my data science skills sharp. I also realized that this would work well as a better explanation of my dissertation also. So my plan is to split everything into three parts. This part is going to discuss stochasticity and how it plays a part of forecasting. The second part will be focused low-code and zero-shot forecasting models. The final part will be marrying these together, like I did in my dissertation, for 9-1-1 centres and how my dissertation can serve as a foundation for future research on both forecasting polystochastic processes and finding even better ways to address forecasting in centers and why it will be important.\n\n\nThis all started with a weird thought playing FreeCell. When I started playing, years ago, I was happy anytime that I had scores under 100 moves. Now, the more Iâ€™ve played and improved, I am annoyed whenever I have a count above 90. That certainly is a big difference. It made me wonder, had I thought of it earlier, could I have built a dataset that charted my progress and then forecast things such as which days are going to be more challenging?\nUltimately, this is a simple stochastic process. The â€œmother-shipâ€ selects a deal at random each day. There are a finite number of choices, according to a quick search, there are \\(1.75^{64}\\) possibilities. The program, as part of the trivia, points out that some deals are unwinnable. So the rules that the â€œmother-shipâ€ must follow also includes, for the daily challenges, the exclusion of unwinnable deals. The program also attaches the daily deal number to the day it is presented, so even if I donâ€™t complete the daily deal, Iâ€™m guaranteed to see the same daily deal that my wife or other players saw the same day. This introduces the stochastic element through the determination of limiting factors. If the daily deal were truly random, then unwinnable deals would be possible and the deal offered would change when presented to the player depending on when it was accessed.\nMy analytical interest, had I started earlier would be to use my prior results to see how my play has improved over time and see if I can forecast how well I can do in the future. I also am curious to see if there is an underlying pattern of when really challenging deals are presented to players. I could correlate my wifeâ€™s results with mine to see if a second pattern emerges. This yields the possibility of two separate types of stochastic forecasts. It also allows me to create pattern recognition analyses that may shed some insight into the algorithm the company employs to select the deal for each day. This works well because my wife and I approach the puzzles in different ways, so if something is challenging to both of our styles, then itâ€™s likely that is really difficult. If it is easy for one of us, the it might not be so difficult. Is it simply a random number generator with some values removed? Or are there other parameters that are added which restrict the lists enough to ensure a player doesnâ€™t perceive repeated deals or perhaps you can ascertain the size of the potential pool of deals.\n\n\n\nThis is most generally found in many commercial call centres. Most accurately, it can be described as a Cox Process. In this scenario, we have a number of calls arriving at a defined rate, but each call is randomly placed and any one call does not impact or effect the next call. The best example to describe a Cox Process in a 9-1-1 centre is that there are a random non-negative number of calls that arrive per time unit, typically an hour. Additionally, the decision of any one individual is neither influenced by or influences the decision of another individual to call the centre.\n\n\nAdditionally, because of the nature of Primary Public Safety Answering Point (PSAP)s, there is an additional level of stochasticity regarding how the call for service arrives to the centre. Calls may arrive from 9-1-1 telephone lines, administrative, non-emergency, lines, Short Message Service (SMS) text messages, Telecommunications Device for the Deaf (TDD)/TeleTYpewriter (TTY) devices, radio communications from officers, or even public walk-ups for centres that have a window for the public, like the county that I live in. This additional stochasticity can magnify the challenges for both forecasting and queue development. So, when weâ€™re trying to build models to describe this behaviour, we have to account for the additional factors that are present.\n\n\n\n\nIn my dissertation, I wanted to examine how to forecast inbound call volumes for PSAPs because itâ€™s the first stage in a larger body of work that I hope to complete. I wanted to find a forecasting model that works well with the technical limitations that can exist in many 9-1-1 centers across the US and Canada. In many of these centers, there arenâ€™t enough telecommunicators on duty, so there also isnâ€™t a lot of technical support available directly to the center. When they need to create forecasts for staffing models, they simply tack on a percentage as an anecdotal guess and then hope that itâ€™s close enough. I was interested in low-code and zero-shot models that could be deployed without incurring substantial technical debt so a center could have a hobbyist who knows just enough Python to make a useful forecast with easy-to-use tools.\nThe biggest problem that I faces in my research process was that I was overcomplicating it. Thankfully, two of my supervisors, Dr.Â Crossland and Dr.Â Schur, saw that and guided me away from turning into somethat I could not handle in a dissertation without it being super long and maybe not completed within the dissertation timeline that the university demanded. When I first proposed the project, I wanted to collect 5 different measurements across hourly, daily, weekly, monthly, and yearly frequencies. My original plan was to collect inbound 9-1-1 trunk line volumes, abandoned 9-1-1 trunk line volumes, administrative line volumes, abandoned administrative line volumes, and outbound call volumes. After I started thinking about that, I also wanted to collect the percentages of calls answered for 9-1-1 and admin trunk lines in 15 and 20 seconds. I had hoped for a grand correlative study where the different volumes could have been seen as exongenous variables to other variables. That would have been unwieldy. Perhaps doing one center to that depth at daily and hourly levels could be a good follow-up study, but this was not the place for it.\nHad I done this study in that way, I might have also reduced the overall stochasticity of the study by discounting that additional level through the separation. the separation could have implied that these calls could be separated prior to arrival at the center. That could be a second interesting study, comparing centers that have dedicated queues solely for administrative calls against those who have singular queues.\n\n\n\nMy next post discussing my dissertation will pick up with the forecasting models, specifically zero-shot and low-code models, and how they may be a better fit for for 9-1-1 centers through the country. After that, I will discuss queueing models and how 9-1-1 centers present challenges to traditional queueing models. Finally, I will discuss application of different popular queueing models, in my opinion, falls short of the promise to model 9-1-1 call centers and what alternatives could exist.\n\n\n\nThe next post that Iâ€™m writing is more fun than work. Itâ€™s an idea that my wife and I started and I think will be a lot of fun in the futureâ€¦."
  },
  {
    "objectID": "posts/20250917/index.html#background",
    "href": "posts/20250917/index.html#background",
    "title": "Stochasticity in Time-Series, Dissertation part 1",
    "section": "",
    "text": "Itâ€™s been nearly a year since I defended my dissertation. In that time, Iâ€™ve found that people are intrested in knowing what itâ€™s about until I try to explain it, then they feel that they donâ€™t understand it and the discussions get lost. So, while I was playing FreeCell solitaire this morning, I thought of an experiment that could help me work with forecasting stochastic processes, including adding conformal predictions to the mixture. Part of it is because I want to keep my data science skills sharp. I also realized that this would work well as a better explanation of my dissertation also. So my plan is to split everything into three parts. This part is going to discuss stochasticity and how it plays a part of forecasting. The second part will be focused low-code and zero-shot forecasting models. The final part will be marrying these together, like I did in my dissertation, for 9-1-1 centres and how my dissertation can serve as a foundation for future research on both forecasting polystochastic processes and finding even better ways to address forecasting in centers and why it will be important.\n\n\nThis all started with a weird thought playing FreeCell. When I started playing, years ago, I was happy anytime that I had scores under 100 moves. Now, the more Iâ€™ve played and improved, I am annoyed whenever I have a count above 90. That certainly is a big difference. It made me wonder, had I thought of it earlier, could I have built a dataset that charted my progress and then forecast things such as which days are going to be more challenging?\nUltimately, this is a simple stochastic process. The â€œmother-shipâ€ selects a deal at random each day. There are a finite number of choices, according to a quick search, there are \\(1.75^{64}\\) possibilities. The program, as part of the trivia, points out that some deals are unwinnable. So the rules that the â€œmother-shipâ€ must follow also includes, for the daily challenges, the exclusion of unwinnable deals. The program also attaches the daily deal number to the day it is presented, so even if I donâ€™t complete the daily deal, Iâ€™m guaranteed to see the same daily deal that my wife or other players saw the same day. This introduces the stochastic element through the determination of limiting factors. If the daily deal were truly random, then unwinnable deals would be possible and the deal offered would change when presented to the player depending on when it was accessed.\nMy analytical interest, had I started earlier would be to use my prior results to see how my play has improved over time and see if I can forecast how well I can do in the future. I also am curious to see if there is an underlying pattern of when really challenging deals are presented to players. I could correlate my wifeâ€™s results with mine to see if a second pattern emerges. This yields the possibility of two separate types of stochastic forecasts. It also allows me to create pattern recognition analyses that may shed some insight into the algorithm the company employs to select the deal for each day. This works well because my wife and I approach the puzzles in different ways, so if something is challenging to both of our styles, then itâ€™s likely that is really difficult. If it is easy for one of us, the it might not be so difficult. Is it simply a random number generator with some values removed? Or are there other parameters that are added which restrict the lists enough to ensure a player doesnâ€™t perceive repeated deals or perhaps you can ascertain the size of the potential pool of deals.\n\n\n\nThis is most generally found in many commercial call centres. Most accurately, it can be described as a Cox Process. In this scenario, we have a number of calls arriving at a defined rate, but each call is randomly placed and any one call does not impact or effect the next call. The best example to describe a Cox Process in a 9-1-1 centre is that there are a random non-negative number of calls that arrive per time unit, typically an hour. Additionally, the decision of any one individual is neither influenced by or influences the decision of another individual to call the centre.\n\n\nAdditionally, because of the nature of Primary Public Safety Answering Point (PSAP)s, there is an additional level of stochasticity regarding how the call for service arrives to the centre. Calls may arrive from 9-1-1 telephone lines, administrative, non-emergency, lines, Short Message Service (SMS) text messages, Telecommunications Device for the Deaf (TDD)/TeleTYpewriter (TTY) devices, radio communications from officers, or even public walk-ups for centres that have a window for the public, like the county that I live in. This additional stochasticity can magnify the challenges for both forecasting and queue development. So, when weâ€™re trying to build models to describe this behaviour, we have to account for the additional factors that are present.\n\n\n\n\nIn my dissertation, I wanted to examine how to forecast inbound call volumes for PSAPs because itâ€™s the first stage in a larger body of work that I hope to complete. I wanted to find a forecasting model that works well with the technical limitations that can exist in many 9-1-1 centers across the US and Canada. In many of these centers, there arenâ€™t enough telecommunicators on duty, so there also isnâ€™t a lot of technical support available directly to the center. When they need to create forecasts for staffing models, they simply tack on a percentage as an anecdotal guess and then hope that itâ€™s close enough. I was interested in low-code and zero-shot models that could be deployed without incurring substantial technical debt so a center could have a hobbyist who knows just enough Python to make a useful forecast with easy-to-use tools.\nThe biggest problem that I faces in my research process was that I was overcomplicating it. Thankfully, two of my supervisors, Dr.Â Crossland and Dr.Â Schur, saw that and guided me away from turning into somethat I could not handle in a dissertation without it being super long and maybe not completed within the dissertation timeline that the university demanded. When I first proposed the project, I wanted to collect 5 different measurements across hourly, daily, weekly, monthly, and yearly frequencies. My original plan was to collect inbound 9-1-1 trunk line volumes, abandoned 9-1-1 trunk line volumes, administrative line volumes, abandoned administrative line volumes, and outbound call volumes. After I started thinking about that, I also wanted to collect the percentages of calls answered for 9-1-1 and admin trunk lines in 15 and 20 seconds. I had hoped for a grand correlative study where the different volumes could have been seen as exongenous variables to other variables. That would have been unwieldy. Perhaps doing one center to that depth at daily and hourly levels could be a good follow-up study, but this was not the place for it.\nHad I done this study in that way, I might have also reduced the overall stochasticity of the study by discounting that additional level through the separation. the separation could have implied that these calls could be separated prior to arrival at the center. That could be a second interesting study, comparing centers that have dedicated queues solely for administrative calls against those who have singular queues.\n\n\n\nMy next post discussing my dissertation will pick up with the forecasting models, specifically zero-shot and low-code models, and how they may be a better fit for for 9-1-1 centers through the country. After that, I will discuss queueing models and how 9-1-1 centers present challenges to traditional queueing models. Finally, I will discuss application of different popular queueing models, in my opinion, falls short of the promise to model 9-1-1 call centers and what alternatives could exist.\n\n\n\nThe next post that Iâ€™m writing is more fun than work. Itâ€™s an idea that my wife and I started and I think will be a lot of fun in the futureâ€¦."
  },
  {
    "objectID": "posts/DeceptiveGraphics/index.html",
    "href": "posts/DeceptiveGraphics/index.html",
    "title": "Deception in Graphics and Statistics",
    "section": "",
    "text": "My wife and I were at our sisterâ€™s place and while we were there, I found a graphic on Facebook that bothered me greatly. I make it a point to ensure that all of my work can rigorously withstand scrutiny. I wonâ€™t tell you something if I canâ€™t show you how I got it and why itâ€™s the way it is. The post, put there by an old high school classmate, showed a graphic that disturbed me because it appeared to be deceptive. This the graphic in question:\n\n\n\nDeceptive Graphic\n\n\nThis graphic appears originally on the blog of The Personal Finance Wizards. This group, per bizapedia.com, is a registered LLC in Arkansas and has the Asa Hutchinson Law Group as its registered agent. So, knowing that Mr.Â Hutchinson is a former Republican Governor of Arkansas, you can expect a conservative bent and misleading information.\nThe LLC describes itself as â€œgroup of personal finance experts dedicated to providing content on money, savings, and investing education.â€. Also, to their credit, their website does have a disclaimer that states:\n\n\n\n\n\n\nWarning\n\n\n\n\nThe Personal Finance Wizards, LLC makes no claim or representation as to the registered trademarks or copyrighted works of others, nor any claim as to the data and tools cited, sourced or referenced herein.\n\n\n\nSo, in essence, â€œWeâ€™ll put this out there, but weâ€™re not responsible if itâ€™s wrong or misleading!â€ That is an ethical problem in my opinion. As my wife and I discussed the graphic, we found more problems with it, along with a lot of people who accepted the intent of the message that comes from the graphic.\nI ended up speaking to my Toastmasters club about the graphic and how it was misleading and deceptive. After discussing the speech with my wife, I decided that I had the material to turn it into a blog post as an example of misleading your audience with statistics and presentation."
  },
  {
    "objectID": "posts/DeceptiveGraphics/index.html#background",
    "href": "posts/DeceptiveGraphics/index.html#background",
    "title": "Deception in Graphics and Statistics",
    "section": "",
    "text": "My wife and I were at our sisterâ€™s place and while we were there, I found a graphic on Facebook that bothered me greatly. I make it a point to ensure that all of my work can rigorously withstand scrutiny. I wonâ€™t tell you something if I canâ€™t show you how I got it and why itâ€™s the way it is. The post, put there by an old high school classmate, showed a graphic that disturbed me because it appeared to be deceptive. This the graphic in question:\n\n\n\nDeceptive Graphic\n\n\nThis graphic appears originally on the blog of The Personal Finance Wizards. This group, per bizapedia.com, is a registered LLC in Arkansas and has the Asa Hutchinson Law Group as its registered agent. So, knowing that Mr.Â Hutchinson is a former Republican Governor of Arkansas, you can expect a conservative bent and misleading information.\nThe LLC describes itself as â€œgroup of personal finance experts dedicated to providing content on money, savings, and investing education.â€. Also, to their credit, their website does have a disclaimer that states:\n\n\n\n\n\n\nWarning\n\n\n\n\nThe Personal Finance Wizards, LLC makes no claim or representation as to the registered trademarks or copyrighted works of others, nor any claim as to the data and tools cited, sourced or referenced herein.\n\n\n\nSo, in essence, â€œWeâ€™ll put this out there, but weâ€™re not responsible if itâ€™s wrong or misleading!â€ That is an ethical problem in my opinion. As my wife and I discussed the graphic, we found more problems with it, along with a lot of people who accepted the intent of the message that comes from the graphic.\nI ended up speaking to my Toastmasters club about the graphic and how it was misleading and deceptive. After discussing the speech with my wife, I decided that I had the material to turn it into a blog post as an example of misleading your audience with statistics and presentation."
  },
  {
    "objectID": "posts/DeceptiveGraphics/index.html#the-problem",
    "href": "posts/DeceptiveGraphics/index.html#the-problem",
    "title": "Deception in Graphics and Statistics",
    "section": "The Problem",
    "text": "The Problem\nThe graphic, as seen above, is a horizontal bar chart that lists different â€œethnicitiesâ€ that receive SNAP benefits. The first problem is how they define â€œethnicityâ€. There is a blending, deliberate, in my opinion of ethnicity, race, and national origin. For example, the first listing in the graphic, Afghani, is misleading because it does not make clear if they are discussing ethnic Pashtuns who comprise about 42% of Afghanis and who may also be called Afghani in country of if it comprises other ethnic groups prevalent in Afghanistan such as Tajiks, Hazaras, Uzbeks, or other ethnic or tribal groups who reside in Afghanistan. Further, this graphic does not distinguish between immigrants from Afghanistan who have been in the US, legally, for more than five years and meet the other eligibility requirements and US Citizens who are ethnically connected to Afghanistan.\nAdditionally, some â€œethnicitiesâ€ such as Arab reflect vastly different groups that are loosely classified as an ethnic group by lingustic heritage, unless they are from a specific country then the nationality is reflected. This appears in the entry for Iraqi, where Arabs, Kurds, Assyrians, Armenians, and Turkmen can all be classified as Iraqis. Additionally, some groups such as the Yazidi, are religious minorities who may be classified as Iraqi.\nIn the graphic, Pacific Islanders are represented by a Hawaiian Flag, which feels very misleading. The Caribbean is listed as a single â€œethnicityâ€, but there are several different ethnic groups who live in the area, and the graphic has separate distinctions for many nationalities in the region, regardless of their ethnicity. LatinX communities are also separated by nationality and not ethnicity.\nFor â€œethnicitiesâ€ represented with an American flag, Indigenous Americans are listed as â€œNativeâ€, African-Americans are listed as â€œBlacksâ€, and then there are, towards the bottom, â€œWhitesâ€. By now, you see the picture. The implication is that immigrants are the problem. This plays fast and loose with the data.\n\nNumerical Deception\nSince neither the grpahic shown above nor the original blog post are clear about how a household is defined and ethnically classified, the percentages are even doubly deceptive. If you use ethnicity and nationality together, several sources including this one place the population at around 200,000 individuals. If all of those individuals were eligible for SNAP benefits, then the eligible population would be about 91,000 people. Thatâ€™s not a significant portion of the population of the United States. In fact, the same group that published this graphic, published a second article three days later that broke the same data down by volume. That graphic, shown below, paints an entirely different picture.\n\n\n\nDeceptive Graphic2\n\n\nIn fact, from this graphic, the Afghani ethnicity does not appear in the list. The number of households is that small. Instead, â€œWhitesâ€, at the bottom of the percentages, just above Italians and Indians, turns out to be the largest in volume. This is where the deliberate deception skews the message. The first published graphic had an agenda. â€œLook at all the foreigners getting things that you donâ€™t.â€ Itâ€™s not true, but it makes for good outrage bait. The group doesnâ€™t define the terms, so they donâ€™t have to clarify what they want to imply. They made the dog whistle at just the right frequency to ensure the point was made and people are distracted from the truth and swallow the deception like pablum.\nAfter publishing both graphics they add text in the second entry to exonerate themselves:\n\n\n\n\n\n\nWarning\n\n\n\n\nItâ€™s important to note that the graph highlights a selection of ethnicities we felt would be most relevant and engaging for our audience. With hundreds of ethnic groups in the U.S., itâ€™s not possible to include every single one in a single post. If there is a group you would like to see represented, let us know or check out the full table available from the U.S. Census.\n\n\n\nSo the second graphic is cherry-picked to both give you some truth, but also continue to stoke the fire from the earlier post. I do appreciate that they added additional informative text:\n\n\n\n\n\n\nNote\n\n\n\n\nItâ€™s also worth noting that we used a logarithmic scale for the y-axis with tick marks at 10K, 100K, 1M, and 10M to better display the wide range in SNAP household counts. This approach helps make the lower numbers like those under 1M visible while still keeping the larger outliers like the White and Black data points in context. Without the log scale, most of the smaller groups would practically be invisible on the graph.\n\n\n\nThis admission is good. We should always make clear to our audience when the scales have been changed for legibility. However, I disagree with their assessment that this keeps those points in context. It merely presents the data in a way to ensure that there is some legibility and counts on the fact that their audience may not read the remainder of the post."
  },
  {
    "objectID": "posts/DeceptiveGraphics/index.html#my-two-cents",
    "href": "posts/DeceptiveGraphics/index.html#my-two-cents",
    "title": "Deception in Graphics and Statistics",
    "section": "My two cents",
    "text": "My two cents\nI think that Iâ€™ve demonstrated how this was a deliberate attempt to misinform an audience and, equally, play to an intended audienceâ€™s biases. Thatâ€™s unethical. I work hard, as do most analysts that I know, to present factual data in a way that ensures the data tells its story honestly. We strive to inform and to give our audiences the tools to make the best decisions. If called upon, I rely on my subject matter knowledge to assist in interpretation, but Iâ€™m clear as to what is my opinion and interpretation and what the data says. There has to be a distinction between the two. These folks didnâ€™t do that. They presented the data in a way that allowed for a misunderstanding of the facts and only after other groups such as Al Jazeera, Politifact, and Wired Magazine called attention to the misinformation did they â€œcorrectâ€ their data. Additionally, they hide behind the disclaimers and caveats. It wasnâ€™t usâ€¦â€¦, we just shared something with you. â€œItâ€™s not our fault if you read something else into it.â€ Again, this is not ethical behaviour. Itâ€™s deceptive and manipulative.\nI will have other posts on here where I talk about my work and I will do my best, each time to fully inform you and ensure that you have the facts and my interpretation. I will also ensure that I make the difference between the information and my interpretation clear. You deserve that.\nThank you for reading this longer post."
  },
  {
    "objectID": "PYTHON_README.html",
    "href": "PYTHON_README.html",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "This site now supports both R and Python code execution and rendering in Quarto documents, using uv for fast and reliable Python environment management.\n\n\n\nuv (install from: https://docs.astral.sh/uv/getting-started/installation/)\nQuarto (install from: https://quarto.org/docs/get-started/)\n\n\n\n\n\n\npython setup_python.py\n\n\n\n# Create virtual environment\nuv venv\n\n# Install all data science packages\nuv pip install pandas numpy matplotlib seaborn scipy statsmodels scikit-learn sqlalchemy pyodbc psycopg2-binary folium geopandas jupyter ipykernel python-dotenv requests plotly altair\n\n# Verify Quarto can detect Python\nsource .venv/bin/activate && quarto check jupyter\n\n\n\n\n\n\n# Render your site (automatically activates environment)\nuv run quarto render\n\n# Preview your site\nuv run quarto preview\n\n# Run Jupyter\nuv run jupyter lab\n\n\n\n# Activate environment\nsource .venv/bin/activate\n\n# Then use quarto normally\nquarto render\nquarto preview\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Your Python code here\ndata = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\nprint(data)\n\n\n\n#| label: my-analysis\n#| echo: true\n#| eval: true\n#| fig-cap: \"My Figure Caption\"\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [4, 5, 6])\nplt.show()\n\n\n\n\n\n#| echo: true/false - Show/hide code\n#| eval: true/false - Execute/donâ€™t execute code\n\n#| output: true/false - Show/hide output\n#| warning: false - Hide warnings\n#| message: false - Hide messages\n#| fig-cap: \"Caption\" - Add figure caption\n#| fig-width: 8 - Set figure width\n#| fig-height: 6 - Set figure height\n\n\n\n\nThe python_utils.py file contains Python equivalents of the R functions used in your analyses:\n\ncustom_summary() - Comprehensive statistics (equivalent to your R function)\nextract_datetime_features() - Extract date/time components\ncreate_response_time_plots() - Generate analysis plots\ncreate_geographic_map() - Interactive maps with Folium\nload_sql_data() - Database connectivity\n\n\n\n\n#| label: load-utilities\nfrom python_utils import custom_summary, create_response_time_plots\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv('cardiac_arrests_cy.csv')\n\n# Get comprehensive statistics\nstats = custom_summary(df['call_entry_time'])\nprint(stats)\n\n# Create visualizations\nfig = create_response_time_plots(df, 'call_entry_time')\nfig.show()\n\n\n\nFor SQL Server connections (equivalent to your R setup):\nimport sqlalchemy as sa\nfrom python_utils import load_sql_data, CARDIAC_ARREST_CURRENT_YEAR\n\n# Connection string for SQL Server\nconn_str = \"mssql+pyodbc://server/database?driver=ODBC+Driver+17+for+SQL+Server\"\n\n# Load data using your SQL queries\ndf = load_sql_data(conn_str, CARDIAC_ARREST_CURRENT_YEAR)\n\n\n\nTo render your site with Python support:\nquarto render\nTo preview with live reload:\nquarto preview\nThe site will automatically detect and execute both R and Python code blocks based on the language specified in the fenced code block."
  },
  {
    "objectID": "PYTHON_README.html#prerequisites",
    "href": "PYTHON_README.html#prerequisites",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "uv (install from: https://docs.astral.sh/uv/getting-started/installation/)\nQuarto (install from: https://quarto.org/docs/get-started/)"
  },
  {
    "objectID": "PYTHON_README.html#setup",
    "href": "PYTHON_README.html#setup",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "python setup_python.py\n\n\n\n# Create virtual environment\nuv venv\n\n# Install all data science packages\nuv pip install pandas numpy matplotlib seaborn scipy statsmodels scikit-learn sqlalchemy pyodbc psycopg2-binary folium geopandas jupyter ipykernel python-dotenv requests plotly altair\n\n# Verify Quarto can detect Python\nsource .venv/bin/activate && quarto check jupyter"
  },
  {
    "objectID": "PYTHON_README.html#using-the-environment",
    "href": "PYTHON_README.html#using-the-environment",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "# Render your site (automatically activates environment)\nuv run quarto render\n\n# Preview your site\nuv run quarto preview\n\n# Run Jupyter\nuv run jupyter lab\n\n\n\n# Activate environment\nsource .venv/bin/activate\n\n# Then use quarto normally\nquarto render\nquarto preview"
  },
  {
    "objectID": "PYTHON_README.html#using-python-in-blog-posts",
    "href": "PYTHON_README.html#using-python-in-blog-posts",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n# Your Python code here\ndata = pd.DataFrame({'x': [1, 2, 3], 'y': [4, 5, 6]})\nprint(data)\n\n\n\n#| label: my-analysis\n#| echo: true\n#| eval: true\n#| fig-cap: \"My Figure Caption\"\n\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [4, 5, 6])\nplt.show()"
  },
  {
    "objectID": "PYTHON_README.html#common-code-block-options",
    "href": "PYTHON_README.html#common-code-block-options",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "#| echo: true/false - Show/hide code\n#| eval: true/false - Execute/donâ€™t execute code\n\n#| output: true/false - Show/hide output\n#| warning: false - Hide warnings\n#| message: false - Hide messages\n#| fig-cap: \"Caption\" - Add figure caption\n#| fig-width: 8 - Set figure width\n#| fig-height: 6 - Set figure height"
  },
  {
    "objectID": "PYTHON_README.html#python-utilities",
    "href": "PYTHON_README.html#python-utilities",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "The python_utils.py file contains Python equivalents of the R functions used in your analyses:\n\ncustom_summary() - Comprehensive statistics (equivalent to your R function)\nextract_datetime_features() - Extract date/time components\ncreate_response_time_plots() - Generate analysis plots\ncreate_geographic_map() - Interactive maps with Folium\nload_sql_data() - Database connectivity"
  },
  {
    "objectID": "PYTHON_README.html#example-usage-in-a-blog-post",
    "href": "PYTHON_README.html#example-usage-in-a-blog-post",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "#| label: load-utilities\nfrom python_utils import custom_summary, create_response_time_plots\nimport pandas as pd\n\n# Load your data\ndf = pd.read_csv('cardiac_arrests_cy.csv')\n\n# Get comprehensive statistics\nstats = custom_summary(df['call_entry_time'])\nprint(stats)\n\n# Create visualizations\nfig = create_response_time_plots(df, 'call_entry_time')\nfig.show()"
  },
  {
    "objectID": "PYTHON_README.html#database-connections",
    "href": "PYTHON_README.html#database-connections",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "For SQL Server connections (equivalent to your R setup):\nimport sqlalchemy as sa\nfrom python_utils import load_sql_data, CARDIAC_ARREST_CURRENT_YEAR\n\n# Connection string for SQL Server\nconn_str = \"mssql+pyodbc://server/database?driver=ODBC+Driver+17+for+SQL+Server\"\n\n# Load data using your SQL queries\ndf = load_sql_data(conn_str, CARDIAC_ARREST_CURRENT_YEAR)"
  },
  {
    "objectID": "PYTHON_README.html#rendering",
    "href": "PYTHON_README.html#rendering",
    "title": "Python Support for Dr.Â Dâ€™s Data Science",
    "section": "",
    "text": "To render your site with Python support:\nquarto render\nTo preview with live reload:\nquarto preview\nThe site will automatically detect and execute both R and Python code blocks based on the language specified in the fenced code block."
  },
  {
    "objectID": "test_python.html",
    "href": "test_python.html",
    "title": "Python Test",
    "section": "",
    "text": "Testing Python execution:\n\nimport pandas as pd\nimport numpy as np\n\n# Test basic functionality\nprint(\"Python is working!\")\nprint(f\"Pandas version: {pd.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n\n# Create a simple DataFrame\ndata = {'x': [1, 2, 3, 4, 5], 'y': [2, 4, 6, 8, 10]}\ndf = pd.DataFrame(data)\nprint(\"\\nSample DataFrame:\")\nprint(df)\n\nPython is working!\nPandas version: 2.3.1\nNumPy version: 2.2.6\n\nSample DataFrame:\n   x   y\n0  1   2\n1  2   4\n2  3   6\n3  4   8\n4  5  10"
  }
]