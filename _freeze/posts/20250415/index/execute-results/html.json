{
  "hash": "3326570c43d4b558ddd8e08fe36b9e6a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How to Start - Asking the First Question\"\nauthor: \"Tony Dunsworth, Ph.D.\"\ndate: \"2025-04-12\"\ncategories: [examples, analyses]\nexecute: \n  eval: false\n---\n\nI recently wrote about a presentation that I gave at [Randolph Macon College](https://rmc.edu) concerning using data analyses in 9-1-1 centres. During the Q&A section, someone asked me how I would recommend getting started. My answer, then and now, is pick a question and dive into that and new questions will start coming. \n\nI thought, perhaps, I should come up with an example of what I mean. In our centre, our medical director has requested that we do quality checks on every cardiac arrest call that we receive. So, here is the starting question: what can we learn about the cardiac arrest calls in the city? With that as the opening question, the first step is collecting the data. To start, I plan on collecting four datasets. I can create all of them using SQL. Since I work on SQL Server or T-SQL flavoured databases, the query, for our dispatch software's databases looks something like this:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 = '2025-01-01 00:00:00.0000000';\n\n-- This query will retrieve all cardiac arrest calls from this year.\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date > @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n\n-- This query will retrieve  all cardiac arrest calls from the past 1, 3, & 5 years\n\nSELECT Master_Incident_Number,\n    Response_Date,\n    Address,\n    Time_PhonePickUp,\n    Time_FirstCallTakingKeystroke,\n    Time_CallEnteredQueue,\n    Time_First_Unit_Assigned,\n    Time_CallTakingComplete\nFROM Response_Master_Incident\nWHERE Response_Date BETWEEN DATEADD(YEAR, -1, @time1) AND @time1\n    AND Problem LIKE 'CARDIAC ARREST%'\n    AND Master_Incident_Number != ''\nORDER BY Response_Date;\n```\n:::\n\n\nThese queries will generate the four datasets that I would want for the full analysis. I would save the output to csv files and name them cardiac_arrest_cy.csv, cardiac_arrest_1y.csv, cardiac_arrest_3y.csv, and cardiac_arrest_5y.csv. \n\nPersonally, I want to start with current data so I can get a feel for the data. To do some of my work, I would add some columns to the dataset. I can do it programmatically or through the SQL Query. I prefer to do it in the query like so:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nUSE Reporting_System;\nGO\n\nDECLARE @time1 DATETIME2;\n\nSET @time1 '2025-01-01';\n\n-- This query will retrieve all cardiac arrest calls from this year\n\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    Address,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n  DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n  DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n  DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date > @time1\nAND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n```\n:::\n\n\nThis gives us a columns of elapsed times to determine how long it took us to make the call dispatchable, how long it took to dispatch the call to the first unit, and how long we spent on the phone with the caller.\n\nNow we load the dataset, for this I'm using the R programming language. We can do it in Python as well, but I've been working with R a lot longer. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_cacy <- read.csv(\"cardiac_arrest_cy.csv\", header = TRUE, sep = \",\", stringsAsFactors = TRUE)\n```\n:::\n\n\nNow that we have the dataset loaded, we can go through the dataset to clean it up. Most of these calls *should* have all of the components that we've selected. If there are things missing, then we can go in and clean those up to remove missing data points. For this dataset, this is the code I would use to clean up any missing values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the data\nstr(df_cacy)\nnrow(df_cacy)\nhead(df_cacy, n = 10)\ncolnames(df_cacy)\nspec(df_cacy)\n\n# Use the naniar package to check for missing values. This creates a graphical view of the missing data\ngg_miss_var(df_cacy)\n\n# Use this code to create a quick table of those missing value counts\napply(X = is.na(df_cacy), MARGIN = 2, FUN = sum)\n\n# This code replaces missing values with an entry\ndf_cacy$Call_ID <- tidyr::replace_na(df_cacy$Call_ID, \"NOT RECORDED\")\ndf_cacy$Start_Time <- tidyr::replace_na(df_cacy$Start_Time, \"1970-01-01 00:00:00\") # This is the Unix Time start value\ndf_cacy$Address <- tidyr::replace_na(df_cacy$Address, \"NOT RECORDED\")\ndf_cacy$Ready_To_Dispatch <- tidyr::replace_na(df_cacy$Ready_To_Dispatch, \"1970-01-01 00:00:00\")\ndf_cacy$First_Unit_Assigned <- tidyr::replace_na(df_cacy$First_Unit_Assigned, \"1970-01-01 00:00:00\")\ndf_cacy$Stop_Time <- tidyr::replace_na(df_cacy$Stop_Time, \"1970-01-01 00:00:00\")\ndf_cacy$Call_Entry_Time <- tidyr::replace_na(df_cacy$Call_Entry_Time, -1) # This makes any elapsed time that is missing a value we can eliminate in the next cleaning step.\ndf_cacy$Call_Queue_Time <- tidyr::replace_na(df_cacy$Call_Queue_Time, -1)\ndf_cacy$Call_Processing_Time <- tidyr::replace_na(df_cacy$Call_Processing_Time, -1)\n```\n:::\n\n\nThis should clean the data of any missing values. This is also the start of new questions. How much data is missing? Where is it missing at? Finally, why is the data missing? Most of the data is likely not missing at random. I expect that most of the missing data comes from calls that were not completed. When the other three benchmark datasets are compared, we can see how the percentage of calls missing data compares over 1, 3, and 5 years. If the comparisons are in line, then you move on, if they aren't, there's question number 4, what's different now?\n\nThe next thing that I check for would be negative values in the final three columns, Call_Entry_Time, Call_Queue_Time, and Call_Processing_Time. These varables represent the elapsed times between events in the centre that measure our telecommunicators actions. If we see any values in these variables that are negative, then we know, instantly, that the calls have been closed then reopened. The fifth questions becomes why are some calls being reopened? My recommendation is to take those calls, and the calls with missing values if they can be proven to be full calls, and create a new dataset. That list can be exported to an Excel spreadsheet and used for future investigation. This is the first potential project that has been generated by this work.\n\n\nDepending on the number of calls that exist in this dataset, I would remove the same calls from my master datasets and continue with the research without them. My benchmark is no more than 5 percent of the calls removed. If it appears that we're going to go over 5 percent, we have another question, why are there so many calls with these defects? \n\n*In a quick glance after running the queries, I found that none of my entries for this year had any missing data and that historical data had missing data in the Time_First_Unit_Assigned column. These would be calls that were received, but never ran for whatever reason. This is what I expected to see in real data.*\n\nOnce the call list has been cleaned, the next step that I recommend is to look at individual variables. I prefer to start with the numeric variables and create a summary. I have a custom summary that I use for my analyses. I feel like it serves my needs and gives me some insights that I will confirm when I create visuals for others. That summary can be found here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Custom summary function\ncustom_summary <- function(column) {\n  c(\n    Minimum = round(min(column, na.rm = TRUE), 2),\n    Mean = round(mean(column, na.rm = TRUE), 2),\n    Median = round(median(column, na.rm = TRUE), 2),\n    Q1 = round(quantile(column, 0.25, na.rm = TRUE), 2),\n    Q3 = round(quantile(column, 0.75, na.rm = TRUE), 2),\n    P90th = round(quantile(column, 0.90. na.rm = TRUE), 2),\n    Maximum = round(max(column, na.rm = TRUE), 2),\n    Std_Dev = round(sd(column, na.rm = TRUE), 2),\n    Variance = round(var(column, na.rm = TRUE), 2),\n    Skewness = round(e1071::skewness(column, na.rm = TRUE), 2),\n    Kurtosis = round(e1071::kurtosis(column, na.rm = TRUE), 2)\n  )\n}\n```\n:::\n\n\nThis custom summary gives me a very good picture of the data prior to creating any visualizations. The minimum and maximum values give me a good range for the data, while the mean and median paint a picture of the central tendencies of the data. The standard deviation and variance can give me some information about the overall spread of the data, the skewness and kurtosis values give me the shape of the distribution, and the Q1 and Q3 values suggest the boundaries for outlier identification. All of this together now gives me a good numeric '*visualization*' of the data and now I can focus on asking new questions of the numeric data.\n\nAdditionally, I will want to break the Response_Date column into some constituent components such as the day of the week, the day of the year, the hour, and the week number. These will allow me to create different pictures. To get those, I typically generate them from the SQL Query. That updated query would look like this:\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT Master_Incident_Number AS [Call_ID],\n    Response_Date AS [Start_Time],\n    CAST(DATEPART(WEEK, Response_Date) AS NVARCHAR(2)) AS [WeekNo],\n    UPPER(FORMAT(Response_Date, 'ddd')) AS [DOW],\n    CAST(DATEPART(DAY, Response_Date) AS NVARCHAR(2)) AS [Day],\n    CAST(DATEPART(Hour, Response_Date) AS NVARCHAR(2)) AS [Hour],\n    Address,\n    Time_PhonePickUp AS [Phone_Start],\n    Time_FirstCallTakingKeystroke AS [Keybiard_Start],\n    Time_CallEnteredQueue AS [Dispatchable],\n    Time_First_Unit_Assigned AS [Dispatched],\n    Time_CallTakingComplete AS [Phone_Stop],\n  DATEDIFF(SECOND, Response_Date, Time_CallEnteredQueue) AS Call_Entry_Time,\n  DATEDIFF(SECOND, Time_CallEnteredQueue, Time_First_Unit_Assigned) AS Call_Queue_Time,\n  DATEDIFF(SECOND, Response_Date, Time_CallTakingComplete) AS Call_Processing_Time\nFROM Response_Master_Incident\nWHERE Response_Date > @time1\nAND  Problem = 'CARDIAC ARREST'\nORDER BY Response_Date;\n```\n:::\n\n\nor you can do it programmatically in R like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to extract day of week (DDD), week number, year, and hour\n# This also uses the lubridate library\nextract_datetime_features <- function(df_cacy, Response_Date) {\n  df %>%\n    mutate(\n      day_of_week_ddd = wday(!!sym(Response_Date), label = TRUE),\n      week_number = isoweek(!!sym(Response_Date)),\n      year_number = year(!!sym(Response_Date)),\n      hour_of_day = hour(!!sym(Response_Date))\n    )\n}\n```\n:::\n\n\nEither way, thsis allows us new data to generate new questions and answers. Is there a specific day of the week that has more events than others? Is there a certain time of the year that has more events? Are there specific hours out of the day that see more events? These questions could impact staffing, training, and could present opportunities to partner with the local hospitals to create better synergies for positive patient experiences and outcomes. So we're now at 8 questions from our starting point as well as a couple of different projects. ",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}